{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndxJqEVEC3c9",
        "outputId": "8fa83ca5-b52e-4f92-d6cf-5338734e60ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- content: string (nullable = true)\n",
            " |-- description: string (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- link: string (nullable = true)\n",
            " |-- pub_date: string (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            "\n",
            "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+-----+-----------------------------------+-------------------------+------------------------------------------------------------------------------+\n",
            "|                                                                         content|                                                                     description|   id|                               link|                 pub_date|                                                                         title|\n",
            "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+-----+-----------------------------------+-------------------------+------------------------------------------------------------------------------+\n",
            "|لأخبار (نواكشوط) استأنف  قطار الشركة الوطنية للصناعة والمناجم اسنيم قبل أقل م...|لأخبار (نواكشوط) استأنف قطار الشركة الوطنية للصناعة والمناجم اسنيم قبل أقل من...|41129|https://alakhbar.info/?q=node/41129|2022-06-21T00:09:59+00:00|                                    بعد إصلاح عطب بالسكك قطار اسنيم يعود للعمل|\n",
            "|الأخبار (نواكشوط) ـ أعلنت اللجنة الفنية المكلفة بمتابعة أعمال اللجنة الوزارية...|الأخبار (نواكشوط) ـ أعلنت اللجنة الفنية المكلفة بمتابعة أعمال اللجنة الوزارية...|41131|https://alakhbar.info/?q=node/41131|2022-06-21T09:13:54+00:00|                       أضرار في الطرق والمساكن بخمس ولايات إثر الأمطار والسيول|\n",
            "|الأخبار (وادان) – عزلت السيول التي تدفقت صباح اليوم من المرتفعات الجبلية مدين...|الأخبار (وادان) – عزلت السيول التي تدفقت صباح اليوم من المرتفعات الجبلية مدين...|41132|https://alakhbar.info/?q=node/41132|2022-06-21T09:51:03+00:00|                        السيول تعزل مدينة ودان وتوقف تنقل السكان منها أو إليها|\n",
            "|الأخبار (وادان) – وصف رئيس حي \"كَافي\" بضاحية مدينة ودان التاريخية اسلاكه ولد ...|الأخبار (وادان) – وصف رئيس حي \"كَافي\" بضاحية مدينة ودان التاريخية اسلاكه ولد ...|41133|https://alakhbar.info/?q=node/41133|2022-06-21T09:54:31+00:00|      رئيس حي \"كَافي\" بوادان: نحن محاصرون وهناك أسر منكوبة بسبب السيول (فيديو)|\n",
            "|الأخبار (وادان) – عزلت السيول التي تدفقت صباح اليوم من المرتفعات الجبلية مدين...|الأخبار (وادان) – عزلت السيول التي تدفقت صباح اليوم من المرتفعات الجبلية مدين...|41134|https://alakhbar.info/?q=node/41134|2022-06-21T10:57:00+00:00|                السيول تعزل مدينة ودان وتوقف تنقل السكان منها أو إليها (فيديو)|\n",
            "|الأخبار (نواكشوط) قال عدد من المنتخبين عن مقاطعة وادان، إن السلطات تسرعت في إ...|الأخبار (نواكشوط) قال عدد من المنتخبين عن مقاطعة وادان، إن السلطات تسرعت في إ...|41135|https://alakhbar.info/?q=node/41135|2022-06-21T12:21:52+00:00|                        منتخبو وادان: السلطات تسرعت في تقييمها لحجم الأضرار   |\n",
            "|الأخبار (نواذيبو)- احتج عدد من النقابيين والبحارة في العاصمة الاقتصادية نواذي...|الأخبار (نواذيبو)- احتج عدد من النقابيين والبحارة في العاصمة الاقتصادية نواذي...|41136|https://alakhbar.info/?q=node/41136|2022-06-21T12:27:11+00:00|                 نقابيون وبحارة يحتجون على إعفاء مدير الدائرة البحرية بنواذيبو|\n",
            "|الأخبار (نواكشوط) ـ أبلغت السفارة المالية في نواكشوط مواطنيها برفع اشتراط رخص...|الأخبار (نواكشوط) ـ أبلغت السفارة المالية في نواكشوط مواطنيها برفع اشتراط رخص...|41137|https://alakhbar.info/?q=node/41137|2022-06-21T12:33:32+00:00|                مالي تبلغ مواطنيها رفع اشتراط رخصة الدخول للعبور إلى موريتانيا|\n",
            "|الأخبار (نواكشوط) أكد الرئيس محمد ولد الغزواني، أن موريتانيا \"تتمتع بموقع است...|الأخبار (نواكشوط) أكد الرئيس محمد ولد الغزواني، أن موريتانيا \"تتمتع بموقع است...|41138|https://alakhbar.info/?q=node/41138|2022-06-21T12:47:00+00:00|                       غزواني: نتمتع بموقع استراتيجي ممتاز ومنفتحون على أوروبا|\n",
            "|الأخبار (نواكشوط) ـ أطلقت السلطة العليا للصحافة والسمعيات البصرية «الهابا» ال...|الأخبار (نواكشوط) ـ أطلقت السلطة العليا للصحافة والسمعيات البصرية «الهابا» ال...|41139|https://alakhbar.info/?q=node/41139|2022-06-21T14:31:51+00:00|                           الهابا تطلق دورة تكوينية حول الصحافة والتحدي الرقمي|\n",
            "|الأخبار (انواكشوط) – قال المزارع لمصطفى ولد بوه وهو أحد سكان بلدية يغرف بولاي...|الأخبار (انواكشوط) – قال المزارع لمصطفى ولد بوه وهو أحد سكان بلدية يغرف بولاي...|41140|https://alakhbar.info/?q=node/41140|2022-06-21T15:54:13+00:00|        مزارع بآدرار: الأودية جاهزة للزراعة والسكان بحاجة لدعم السلطات (فيديو)|\n",
            "|سأتحدث قليلا عن فريقنـا المشارك في البطولة الدولية لمناظرات الجامعات، والحديث...|سأتحدث قليلا عن فريقنـا المشارك في البطولة الدولية لمناظرات الجامعات، والحديث...|41141|https://alakhbar.info/?q=node/41141|2022-06-21T16:09:43+00:00|                                                       معاناة في سبيل المواطنة|\n",
            "|الأخبار نواكشوط – خلفت الأمطار الغزيرة خلال اليومين الماضيين على بلدية \"يقرف\"...|الأخبار نواكشوط – خلفت الأمطار الغزيرة خلال اليومين الماضيين على بلدية \"يقرف\"...|41142|https://alakhbar.info/?q=node/41142|2022-06-21T16:37:40+00:00|                                        الأمطار تخلف أضرارا مادية بقرية \"يقرف\"|\n",
            "|*ضاد السياسة  (3)* - ( ياخير من بذل الندى ببنانه وعلا على العليا بنعل جواد ) ...|*ضاد السياسة (3)* - ( ياخير من بذل الندى ببنانه وعلا على العليا بنعل جواد ) (...|41144|https://alakhbar.info/?q=node/41144|2022-06-21T18:22:28+00:00|                                                              الهلابج والتكعيب|\n",
            "|الأخبار (نواكشوط) – توفى مساء اليوم فتى يبلغ من العمر 18 سنة تقريبا مساء اليو...|الأخبار (نواكشوط) – توفى مساء اليوم فتى يبلغ من العمر 18 سنة تقريبا مساء اليو...|41145|https://alakhbar.info/?q=node/41145|2022-06-21T22:33:36+00:00|                                                   وفاة فتى غرقا في مدينة أطار|\n",
            "|الأخبار (نواكشوط) عرفت الأربع والعشرين ساعة الأخيرة تهاطل أمطار على 10 ولايات...|الأخبار (نواكشوط) عرفت الأربع والعشرين ساعة الأخيرة تهاطل أمطار على 10 ولايات...|41143|https://alakhbar.info/?q=node/41143|2022-06-21T16:47:11+00:00|                                    أمطار بـ 10 ولايات بلغت 90 مم ببعض المناطق|\n",
            "|الأخبار (نواكشوط) أعلن اتحاد مكاتب الجاليات الموريتانية في العالم اليوم الثلا...|الأخبار (نواكشوط) أعلن اتحاد مكاتب الجاليات الموريتانية في العالم اليوم الثلا...|41146|https://alakhbar.info/?q=node/41146|2022-06-21T23:19:09+00:00|                      اتحاد الجاليات يعلن حل مشكلة استصدار جوازات السفر عن بعد|\n",
            "|الأخبار (نواكشوط) – قال عمدة بلدية معدن العرفان بولاية آدرار عمو ولد الدهاه إ...|الأخبار (نواكشوط) – قال عمدة بلدية معدن العرفان بولاية آدرار عمو ولد الدهاه إ...|41151|https://alakhbar.info/?q=node/41151|2022-06-22T03:20:26+00:00|     عمدة المعدن للأخبار: الأمطار خلفت أضرارا متعددة ونطالب بتدخل عاجل (فيديو)|\n",
            "|الأخبار (وادان) – وصف حاكم مقاطعة وادان في ولاية آدرار محمد حرمه محمد قلي أحم...|الأخبار (وادان) – وصف حاكم مقاطعة وادان في ولاية آدرار محمد حرمه محمد قلي أحم...|41148|https://alakhbar.info/?q=node/41148|2022-06-22T02:04:45+00:00|حاكم وادان للأخبار: وضعية المدينة طبيعية ولم تتأثر فيها أي خدمة أساسية (فيديو)|\n",
            "|                                                                                |                                                                            NULL|41149|https://alakhbar.info/?q=node/41149|2022-06-22T02:10:33+00:00|        حاكم وادان للأخبار: وضعية المدينة طبيعية ولم تتأثر فيها أي خدمة أساسية|\n",
            "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+-----+-----------------------------------+-------------------------+------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----+-------+-------------------------+------------------------------------------------------------------------------+\n",
            "|   id|cluster|                 pub_date|                                                                         title|\n",
            "+-----+-------+-------------------------+------------------------------------------------------------------------------+\n",
            "|41129|      0|2022-06-21T00:09:59+00:00|                                    بعد إصلاح عطب بالسكك قطار اسنيم يعود للعمل|\n",
            "|41131|      0|2022-06-21T09:13:54+00:00|                       أضرار في الطرق والمساكن بخمس ولايات إثر الأمطار والسيول|\n",
            "|41132|      0|2022-06-21T09:51:03+00:00|                        السيول تعزل مدينة ودان وتوقف تنقل السكان منها أو إليها|\n",
            "|41133|      0|2022-06-21T09:54:31+00:00|      رئيس حي \"كَافي\" بوادان: نحن محاصرون وهناك أسر منكوبة بسبب السيول (فيديو)|\n",
            "|41134|      0|2022-06-21T10:57:00+00:00|                السيول تعزل مدينة ودان وتوقف تنقل السكان منها أو إليها (فيديو)|\n",
            "|41135|      0|2022-06-21T12:21:52+00:00|                        منتخبو وادان: السلطات تسرعت في تقييمها لحجم الأضرار   |\n",
            "|41136|      0|2022-06-21T12:27:11+00:00|                 نقابيون وبحارة يحتجون على إعفاء مدير الدائرة البحرية بنواذيبو|\n",
            "|41137|      0|2022-06-21T12:33:32+00:00|                مالي تبلغ مواطنيها رفع اشتراط رخصة الدخول للعبور إلى موريتانيا|\n",
            "|41138|      0|2022-06-21T12:47:00+00:00|                       غزواني: نتمتع بموقع استراتيجي ممتاز ومنفتحون على أوروبا|\n",
            "|41139|      0|2022-06-21T14:31:51+00:00|                           الهابا تطلق دورة تكوينية حول الصحافة والتحدي الرقمي|\n",
            "|41140|      0|2022-06-21T15:54:13+00:00|        مزارع بآدرار: الأودية جاهزة للزراعة والسكان بحاجة لدعم السلطات (فيديو)|\n",
            "|41141|      0|2022-06-21T16:09:43+00:00|                                                       معاناة في سبيل المواطنة|\n",
            "|41142|      0|2022-06-21T16:37:40+00:00|                                        الأمطار تخلف أضرارا مادية بقرية \"يقرف\"|\n",
            "|41144|      0|2022-06-21T18:22:28+00:00|                                                              الهلابج والتكعيب|\n",
            "|41145|      0|2022-06-21T22:33:36+00:00|                                                   وفاة فتى غرقا في مدينة أطار|\n",
            "|41143|      2|2022-06-21T16:47:11+00:00|                                    أمطار بـ 10 ولايات بلغت 90 مم ببعض المناطق|\n",
            "|41146|      0|2022-06-21T23:19:09+00:00|                      اتحاد الجاليات يعلن حل مشكلة استصدار جوازات السفر عن بعد|\n",
            "|41151|      0|2022-06-22T03:20:26+00:00|     عمدة المعدن للأخبار: الأمطار خلفت أضرارا متعددة ونطالب بتدخل عاجل (فيديو)|\n",
            "|41148|      0|2022-06-22T02:04:45+00:00|حاكم وادان للأخبار: وضعية المدينة طبيعية ولم تتأثر فيها أي خدمة أساسية (فيديو)|\n",
            "|41149|      0|2022-06-22T02:10:33+00:00|        حاكم وادان للأخبار: وضعية المدينة طبيعية ولم تتأثر فيها أي خدمة أساسية|\n",
            "+-----+-------+-------------------------+------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Silhouette Score: 0.9297278041790793\n",
            "+-----+-----------------------------------------------------------+-------+\n",
            "|   id|                                                      title|cluster|\n",
            "+-----+-----------------------------------------------------------+-------+\n",
            "|58999|وزير الدفاع يغادر إلى التشيك ضمن جولة خارجية تستمر 8 أيام  |      0|\n",
            "|58998| موريتانيا والكويت تبحثان التعاون في إدارة المخاطر والكوارث|      0|\n",
            "+-----+-----------------------------------------------------------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import concat_ws, to_timestamp, col\n",
        "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, IDF\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"NewsArticleCategorization\").getOrCreate()\n",
        "\n",
        "# Load the dataset (adjust the file path as needed)\n",
        "data = spark.read.json(\"alakhbar.json\")\n",
        "data.printSchema()\n",
        "data.show(truncate=80)\n",
        "\n",
        "# Combine text fields (title, description, content) into a single column \"text\"\n",
        "data = data.withColumn(\"text\", concat_ws(\" \", \"title\", \"description\", \"content\"))\n",
        "\n",
        "# Preprocessing: Tokenize the text using RegexTokenizer\n",
        "tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
        "\n",
        "# Remove stop words (for Arabic, you may need to supply a custom list of Arabic stop words)\n",
        "arabic_stopwords = [\"في\", \"من\", \"على\", \"و\", \"إلى\", \"عن\", \"أن\", \"كان\", \"مع\", \"هذا\", \"هذه\", \"لم\", \"لا\", \"ما\"]  # Extend as needed\n",
        "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", stopWords=arabic_stopwords)\n",
        "\n",
        "# Feature Extraction: Convert words to term frequency vectors\n",
        "vectorizer = CountVectorizer(inputCol=\"filtered\", outputCol=\"rawFeatures\", vocabSize=10000, minDF=2)\n",
        "\n",
        "# Compute TF-IDF from the term frequency vectors\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "\n",
        "# Clustering: Use KMeans with k=5 (predefined number of categories)\n",
        "kmeans = KMeans(featuresCol=\"features\", predictionCol=\"cluster\", k=5, seed=42)\n",
        "\n",
        "# Create a pipeline to streamline the steps\n",
        "pipeline = Pipeline(stages=[tokenizer, stopwords_remover, vectorizer, idf, kmeans])\n",
        "\n",
        "# Train the pipeline model on the entire dataset\n",
        "model = pipeline.fit(data)\n",
        "\n",
        "# Transform the dataset to get clusters\n",
        "clustered_data = model.transform(data)\n",
        "clustered_data.select(\"id\", \"cluster\", \"pub_date\", \"title\").show(truncate=80)\n",
        "\n",
        "# Evaluate clustering quality using Silhouette Score\n",
        "evaluator = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"cluster\", metricName=\"silhouette\", distanceMeasure=\"squaredEuclidean\")\n",
        "silhouette = evaluator.evaluate(clustered_data)\n",
        "print(f\"Silhouette Score: {silhouette}\")\n",
        "\n",
        "# OPTIONAL: Analyze top terms per cluster by exploring the vocabulary\n",
        "# (This might involve extracting cluster centers and mapping indices to words from vectorizer.vocabulary)\n",
        "\n",
        "# Demonstration: Classify two most recent articles based on pub_date\n",
        "# Convert pub_date to timestamp if not already\n",
        "data = data.withColumn(\"timestamp\", to_timestamp(\"pub_date\"))\n",
        "recent_articles = data.orderBy(col(\"timestamp\").desc()).limit(2)\n",
        "recent_predictions = model.transform(recent_articles)\n",
        "recent_predictions.select(\"id\", \"title\", \"cluster\").show(truncate=80)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import concat_ws, to_timestamp, col\n",
        "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, IDF\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "\n",
        "# 1. Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"NewsArticleCategorization\").getOrCreate()\n",
        "\n",
        "# 2. Load the dataset from a JSON file (update the file path accordingly)\n",
        "data = spark.read.json(\"alakhbar.json\")\n",
        "data.printSchema()  # Verify the schema\n",
        "data.show(truncate=80)  # Preview the dataset\n",
        "\n",
        "# 3. Combine text fields (title, description, content) into one column \"text\"\n",
        "data = data.withColumn(\"text\", concat_ws(\" \", \"title\", \"description\", \"content\"))\n",
        "\n",
        "# 4. Text Preprocessing\n",
        "# 4a. Tokenization: Split the combined text into words\n",
        "tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
        "\n",
        "# 4b. Remove Stop Words: Use a custom list for Arabic (you can extend this list as needed)\n",
        "arabic_stopwords = [\"في\", \"من\", \"على\", \"و\", \"إلى\", \"عن\", \"أن\", \"كان\", \"مع\", \"هذا\", \"هذه\", \"لم\", \"لا\", \"ما\"]\n",
        "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", stopWords=arabic_stopwords)\n",
        "\n",
        "# 5. Feature Extraction\n",
        "# 5a. CountVectorizer: Transform filtered tokens into numerical term frequency vectors\n",
        "vectorizer = CountVectorizer(inputCol=\"filtered\", outputCol=\"rawFeatures\", vocabSize=10000, minDF=2)\n",
        "\n",
        "# 5b. TF-IDF: Scale the term frequency vectors to emphasize important words\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "\n",
        "# 6. Clustering: Using KMeans to group articles into 5 clusters (k=5)\n",
        "kmeans = KMeans(featuresCol=\"features\", predictionCol=\"cluster\", k=5, seed=42)\n",
        "\n",
        "# 7. Create a Pipeline to chain all steps together\n",
        "pipeline = Pipeline(stages=[tokenizer, stopwords_remover, vectorizer, idf, kmeans])\n",
        "\n",
        "# 8. Train the model on the dataset\n",
        "model = pipeline.fit(data)\n",
        "\n",
        "# 9. Transform the data to assign clusters to each article\n",
        "clustered_data = model.transform(data)\n",
        "clustered_data.select(\"id\", \"cluster\", \"pub_date\", \"title\").show(truncate=80)\n",
        "\n",
        "# 10. Evaluate the clustering performance using the Silhouette Score\n",
        "evaluator = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"cluster\",\n",
        "                                metricName=\"silhouette\", distanceMeasure=\"squaredEuclidean\")\n",
        "silhouette = evaluator.evaluate(clustered_data)\n",
        "print(\"Silhouette Score:\", silhouette)\n",
        "\n",
        "# 11. Demonstration: Classify two of the most recent articles\n",
        "# Convert pub_date to timestamp for ordering\n",
        "data = data.withColumn(\"timestamp\", to_timestamp(\"pub_date\"))\n",
        "# Order articles by the most recent publication date and select two\n",
        "recent_articles = data.orderBy(col(\"timestamp\").desc()).limit(2)\n",
        "# Use the trained pipeline model to predict the cluster for these articles\n",
        "recent_predictions = model.transform(recent_articles)\n",
        "recent_predictions.select(\"id\", \"title\", \"cluster\").show(truncate=80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_C17ZaeTiBM",
        "outputId": "9dc3e8f1-d8a4-4612-a33f-127cf23bd4b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- content: string (nullable = true)\n",
            " |-- description: string (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- link: string (nullable = true)\n",
            " |-- pub_date: string (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            "\n",
            "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+-----+-----------------------------------+-------------------------+------------------------------------------------------------------------------+\n",
            "|                                                                         content|                                                                     description|   id|                               link|                 pub_date|                                                                         title|\n",
            "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+-----+-----------------------------------+-------------------------+------------------------------------------------------------------------------+\n",
            "|لأخبار (نواكشوط) استأنف  قطار الشركة الوطنية للصناعة والمناجم اسنيم قبل أقل م...|لأخبار (نواكشوط) استأنف قطار الشركة الوطنية للصناعة والمناجم اسنيم قبل أقل من...|41129|https://alakhbar.info/?q=node/41129|2022-06-21T00:09:59+00:00|                                    بعد إصلاح عطب بالسكك قطار اسنيم يعود للعمل|\n",
            "|الأخبار (نواكشوط) ـ أعلنت اللجنة الفنية المكلفة بمتابعة أعمال اللجنة الوزارية...|الأخبار (نواكشوط) ـ أعلنت اللجنة الفنية المكلفة بمتابعة أعمال اللجنة الوزارية...|41131|https://alakhbar.info/?q=node/41131|2022-06-21T09:13:54+00:00|                       أضرار في الطرق والمساكن بخمس ولايات إثر الأمطار والسيول|\n",
            "|الأخبار (وادان) – عزلت السيول التي تدفقت صباح اليوم من المرتفعات الجبلية مدين...|الأخبار (وادان) – عزلت السيول التي تدفقت صباح اليوم من المرتفعات الجبلية مدين...|41132|https://alakhbar.info/?q=node/41132|2022-06-21T09:51:03+00:00|                        السيول تعزل مدينة ودان وتوقف تنقل السكان منها أو إليها|\n",
            "|الأخبار (وادان) – وصف رئيس حي \"كَافي\" بضاحية مدينة ودان التاريخية اسلاكه ولد ...|الأخبار (وادان) – وصف رئيس حي \"كَافي\" بضاحية مدينة ودان التاريخية اسلاكه ولد ...|41133|https://alakhbar.info/?q=node/41133|2022-06-21T09:54:31+00:00|      رئيس حي \"كَافي\" بوادان: نحن محاصرون وهناك أسر منكوبة بسبب السيول (فيديو)|\n",
            "|الأخبار (وادان) – عزلت السيول التي تدفقت صباح اليوم من المرتفعات الجبلية مدين...|الأخبار (وادان) – عزلت السيول التي تدفقت صباح اليوم من المرتفعات الجبلية مدين...|41134|https://alakhbar.info/?q=node/41134|2022-06-21T10:57:00+00:00|                السيول تعزل مدينة ودان وتوقف تنقل السكان منها أو إليها (فيديو)|\n",
            "|الأخبار (نواكشوط) قال عدد من المنتخبين عن مقاطعة وادان، إن السلطات تسرعت في إ...|الأخبار (نواكشوط) قال عدد من المنتخبين عن مقاطعة وادان، إن السلطات تسرعت في إ...|41135|https://alakhbar.info/?q=node/41135|2022-06-21T12:21:52+00:00|                        منتخبو وادان: السلطات تسرعت في تقييمها لحجم الأضرار   |\n",
            "|الأخبار (نواذيبو)- احتج عدد من النقابيين والبحارة في العاصمة الاقتصادية نواذي...|الأخبار (نواذيبو)- احتج عدد من النقابيين والبحارة في العاصمة الاقتصادية نواذي...|41136|https://alakhbar.info/?q=node/41136|2022-06-21T12:27:11+00:00|                 نقابيون وبحارة يحتجون على إعفاء مدير الدائرة البحرية بنواذيبو|\n",
            "|الأخبار (نواكشوط) ـ أبلغت السفارة المالية في نواكشوط مواطنيها برفع اشتراط رخص...|الأخبار (نواكشوط) ـ أبلغت السفارة المالية في نواكشوط مواطنيها برفع اشتراط رخص...|41137|https://alakhbar.info/?q=node/41137|2022-06-21T12:33:32+00:00|                مالي تبلغ مواطنيها رفع اشتراط رخصة الدخول للعبور إلى موريتانيا|\n",
            "|الأخبار (نواكشوط) أكد الرئيس محمد ولد الغزواني، أن موريتانيا \"تتمتع بموقع است...|الأخبار (نواكشوط) أكد الرئيس محمد ولد الغزواني، أن موريتانيا \"تتمتع بموقع است...|41138|https://alakhbar.info/?q=node/41138|2022-06-21T12:47:00+00:00|                       غزواني: نتمتع بموقع استراتيجي ممتاز ومنفتحون على أوروبا|\n",
            "|الأخبار (نواكشوط) ـ أطلقت السلطة العليا للصحافة والسمعيات البصرية «الهابا» ال...|الأخبار (نواكشوط) ـ أطلقت السلطة العليا للصحافة والسمعيات البصرية «الهابا» ال...|41139|https://alakhbar.info/?q=node/41139|2022-06-21T14:31:51+00:00|                           الهابا تطلق دورة تكوينية حول الصحافة والتحدي الرقمي|\n",
            "|الأخبار (انواكشوط) – قال المزارع لمصطفى ولد بوه وهو أحد سكان بلدية يغرف بولاي...|الأخبار (انواكشوط) – قال المزارع لمصطفى ولد بوه وهو أحد سكان بلدية يغرف بولاي...|41140|https://alakhbar.info/?q=node/41140|2022-06-21T15:54:13+00:00|        مزارع بآدرار: الأودية جاهزة للزراعة والسكان بحاجة لدعم السلطات (فيديو)|\n",
            "|سأتحدث قليلا عن فريقنـا المشارك في البطولة الدولية لمناظرات الجامعات، والحديث...|سأتحدث قليلا عن فريقنـا المشارك في البطولة الدولية لمناظرات الجامعات، والحديث...|41141|https://alakhbar.info/?q=node/41141|2022-06-21T16:09:43+00:00|                                                       معاناة في سبيل المواطنة|\n",
            "|الأخبار نواكشوط – خلفت الأمطار الغزيرة خلال اليومين الماضيين على بلدية \"يقرف\"...|الأخبار نواكشوط – خلفت الأمطار الغزيرة خلال اليومين الماضيين على بلدية \"يقرف\"...|41142|https://alakhbar.info/?q=node/41142|2022-06-21T16:37:40+00:00|                                        الأمطار تخلف أضرارا مادية بقرية \"يقرف\"|\n",
            "|*ضاد السياسة  (3)* - ( ياخير من بذل الندى ببنانه وعلا على العليا بنعل جواد ) ...|*ضاد السياسة (3)* - ( ياخير من بذل الندى ببنانه وعلا على العليا بنعل جواد ) (...|41144|https://alakhbar.info/?q=node/41144|2022-06-21T18:22:28+00:00|                                                              الهلابج والتكعيب|\n",
            "|الأخبار (نواكشوط) – توفى مساء اليوم فتى يبلغ من العمر 18 سنة تقريبا مساء اليو...|الأخبار (نواكشوط) – توفى مساء اليوم فتى يبلغ من العمر 18 سنة تقريبا مساء اليو...|41145|https://alakhbar.info/?q=node/41145|2022-06-21T22:33:36+00:00|                                                   وفاة فتى غرقا في مدينة أطار|\n",
            "|الأخبار (نواكشوط) عرفت الأربع والعشرين ساعة الأخيرة تهاطل أمطار على 10 ولايات...|الأخبار (نواكشوط) عرفت الأربع والعشرين ساعة الأخيرة تهاطل أمطار على 10 ولايات...|41143|https://alakhbar.info/?q=node/41143|2022-06-21T16:47:11+00:00|                                    أمطار بـ 10 ولايات بلغت 90 مم ببعض المناطق|\n",
            "|الأخبار (نواكشوط) أعلن اتحاد مكاتب الجاليات الموريتانية في العالم اليوم الثلا...|الأخبار (نواكشوط) أعلن اتحاد مكاتب الجاليات الموريتانية في العالم اليوم الثلا...|41146|https://alakhbar.info/?q=node/41146|2022-06-21T23:19:09+00:00|                      اتحاد الجاليات يعلن حل مشكلة استصدار جوازات السفر عن بعد|\n",
            "|الأخبار (نواكشوط) – قال عمدة بلدية معدن العرفان بولاية آدرار عمو ولد الدهاه إ...|الأخبار (نواكشوط) – قال عمدة بلدية معدن العرفان بولاية آدرار عمو ولد الدهاه إ...|41151|https://alakhbar.info/?q=node/41151|2022-06-22T03:20:26+00:00|     عمدة المعدن للأخبار: الأمطار خلفت أضرارا متعددة ونطالب بتدخل عاجل (فيديو)|\n",
            "|الأخبار (وادان) – وصف حاكم مقاطعة وادان في ولاية آدرار محمد حرمه محمد قلي أحم...|الأخبار (وادان) – وصف حاكم مقاطعة وادان في ولاية آدرار محمد حرمه محمد قلي أحم...|41148|https://alakhbar.info/?q=node/41148|2022-06-22T02:04:45+00:00|حاكم وادان للأخبار: وضعية المدينة طبيعية ولم تتأثر فيها أي خدمة أساسية (فيديو)|\n",
            "|                                                                                |                                                                            NULL|41149|https://alakhbar.info/?q=node/41149|2022-06-22T02:10:33+00:00|        حاكم وادان للأخبار: وضعية المدينة طبيعية ولم تتأثر فيها أي خدمة أساسية|\n",
            "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+-----+-----------------------------------+-------------------------+------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----+-------+-------------------------+------------------------------------------------------------------------------+\n",
            "|   id|cluster|                 pub_date|                                                                         title|\n",
            "+-----+-------+-------------------------+------------------------------------------------------------------------------+\n",
            "|41129|      0|2022-06-21T00:09:59+00:00|                                    بعد إصلاح عطب بالسكك قطار اسنيم يعود للعمل|\n",
            "|41131|      0|2022-06-21T09:13:54+00:00|                       أضرار في الطرق والمساكن بخمس ولايات إثر الأمطار والسيول|\n",
            "|41132|      0|2022-06-21T09:51:03+00:00|                        السيول تعزل مدينة ودان وتوقف تنقل السكان منها أو إليها|\n",
            "|41133|      0|2022-06-21T09:54:31+00:00|      رئيس حي \"كَافي\" بوادان: نحن محاصرون وهناك أسر منكوبة بسبب السيول (فيديو)|\n",
            "|41134|      0|2022-06-21T10:57:00+00:00|                السيول تعزل مدينة ودان وتوقف تنقل السكان منها أو إليها (فيديو)|\n",
            "|41135|      0|2022-06-21T12:21:52+00:00|                        منتخبو وادان: السلطات تسرعت في تقييمها لحجم الأضرار   |\n",
            "|41136|      0|2022-06-21T12:27:11+00:00|                 نقابيون وبحارة يحتجون على إعفاء مدير الدائرة البحرية بنواذيبو|\n",
            "|41137|      0|2022-06-21T12:33:32+00:00|                مالي تبلغ مواطنيها رفع اشتراط رخصة الدخول للعبور إلى موريتانيا|\n",
            "|41138|      0|2022-06-21T12:47:00+00:00|                       غزواني: نتمتع بموقع استراتيجي ممتاز ومنفتحون على أوروبا|\n",
            "|41139|      0|2022-06-21T14:31:51+00:00|                           الهابا تطلق دورة تكوينية حول الصحافة والتحدي الرقمي|\n",
            "|41140|      0|2022-06-21T15:54:13+00:00|        مزارع بآدرار: الأودية جاهزة للزراعة والسكان بحاجة لدعم السلطات (فيديو)|\n",
            "|41141|      0|2022-06-21T16:09:43+00:00|                                                       معاناة في سبيل المواطنة|\n",
            "|41142|      0|2022-06-21T16:37:40+00:00|                                        الأمطار تخلف أضرارا مادية بقرية \"يقرف\"|\n",
            "|41144|      0|2022-06-21T18:22:28+00:00|                                                              الهلابج والتكعيب|\n",
            "|41145|      0|2022-06-21T22:33:36+00:00|                                                   وفاة فتى غرقا في مدينة أطار|\n",
            "|41143|      2|2022-06-21T16:47:11+00:00|                                    أمطار بـ 10 ولايات بلغت 90 مم ببعض المناطق|\n",
            "|41146|      0|2022-06-21T23:19:09+00:00|                      اتحاد الجاليات يعلن حل مشكلة استصدار جوازات السفر عن بعد|\n",
            "|41151|      0|2022-06-22T03:20:26+00:00|     عمدة المعدن للأخبار: الأمطار خلفت أضرارا متعددة ونطالب بتدخل عاجل (فيديو)|\n",
            "|41148|      0|2022-06-22T02:04:45+00:00|حاكم وادان للأخبار: وضعية المدينة طبيعية ولم تتأثر فيها أي خدمة أساسية (فيديو)|\n",
            "|41149|      0|2022-06-22T02:10:33+00:00|        حاكم وادان للأخبار: وضعية المدينة طبيعية ولم تتأثر فيها أي خدمة أساسية|\n",
            "+-----+-------+-------------------------+------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Silhouette Score: 0.9297278041790793\n",
            "+-----+-----------------------------------------------------------+-------+\n",
            "|   id|                                                      title|cluster|\n",
            "+-----+-----------------------------------------------------------+-------+\n",
            "|58999|وزير الدفاع يغادر إلى التشيك ضمن جولة خارجية تستمر 8 أيام  |      0|\n",
            "|58998| موريتانيا والكويت تبحثان التعاون في إدارة المخاطر والكوارث|      0|\n",
            "+-----+-----------------------------------------------------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import concat_ws\n",
        "\n",
        "# New article details\n",
        "new_title = \"وزير الخارجية يعقد مؤتمر صحفي حول العلاقات الثنائية\"\n",
        "\n",
        "# Create a DataFrame with the expected columns\n",
        "new_article = spark.createDataFrame(\n",
        "    [(new_title, \"\", \"\")],\n",
        "    [\"title\", \"description\", \"content\"]\n",
        ")\n",
        "\n",
        "# Create the \"text\" column exactly as done during training\n",
        "new_article = new_article.withColumn(\"text\", concat_ws(\" \", \"title\", \"description\", \"content\"))\n",
        "\n",
        "# Use the trained model's pipeline to predict the cluster for this new article\n",
        "prediction = model.transform(new_article)\n",
        "\n",
        "# Show the predicted cluster label along with the title\n",
        "prediction.select(\"title\", \"cluster\").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqA562W4UQDB",
        "outputId": "64950206-8b59-4db2-8e1b-9aea48e3e706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------+-------+\n",
            "|title                                              |cluster|\n",
            "+---------------------------------------------------+-------+\n",
            "|وزير الخارجية يعقد مؤتمر صحفي حول العلاقات الثنائية|0      |\n",
            "+---------------------------------------------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import concat_ws, col, udf, collect_list, expr\n",
        "from pyspark.sql.types import StringType, ArrayType\n",
        "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
        "from pyspark.ml.clustering import LDA\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ArabicNewsTopicModeling\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# 1. Data Loading\n",
        "# Replace with the path to your dataset\n",
        "data = spark.read.json(\"alakhbar.json\")\n",
        "print(\"Dataset Schema:\")\n",
        "data.printSchema()\n",
        "print(\"Sample Data:\")\n",
        "data.show(5, truncate=80)\n",
        "\n",
        "# 2. Text Preprocessing for Arabic\n",
        "# 2.1 Combine fields into one text field\n",
        "data = data.withColumn(\"text\", concat_ws(\" \", col(\"title\"), col(\"description\"), col(\"content\")))\n",
        "\n",
        "# 2.2 Arabic Text Normalization and Cleaning\n",
        "def normalize_arabic(text):\n",
        "    \"\"\"Normalize Arabic text by removing diacritics, normalizing forms, and keeping only Arabic characters\"\"\"\n",
        "    if text is None:\n",
        "        return \"\"\n",
        "\n",
        "    # Normalize Alef variations\n",
        "    text = text.replace(\"أ\", \"ا\").replace(\"إ\", \"ا\").replace(\"آ\", \"ا\")\n",
        "\n",
        "    # Normalize Ya variations\n",
        "    text = text.replace(\"ى\", \"ي\")\n",
        "\n",
        "    # Remove diacritics and tatweel\n",
        "    diac_pattern = re.compile(r'[\\u064B-\\u065F\\u0640]')\n",
        "    text = re.sub(diac_pattern, '', text)\n",
        "\n",
        "    # Keep only Arabic letters and spaces\n",
        "    arabic_pattern = re.compile(r'[^\\u0600-\\u06FF\\s]')\n",
        "    text = re.sub(arabic_pattern, ' ', text)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "normalize_udf = udf(normalize_arabic, StringType())\n",
        "data = data.withColumn(\"norm_text\", normalize_udf(col(\"text\")))\n",
        "\n",
        "# 2.3 Tokenization - Using a pattern that better preserves Arabic words\n",
        "tokenizer = RegexTokenizer(inputCol=\"norm_text\", outputCol=\"words\", pattern=\"[^\\\\p{L}\\\\p{N}_]+\", gaps=True)\n",
        "tokenized_data = tokenizer.transform(data)\n",
        "\n",
        "# 2.4 Remove Arabic stopwords\n",
        "# Extended list of Arabic stopwords\n",
        "arabic_stopwords = [\n",
        "    \"في\", \"من\", \"على\", \"إلى\", \"عن\", \"مع\", \"هذا\", \"هذه\", \"أن\", \"و\", \"ب\", \"ل\", \"الى\",\n",
        "    \"إن\", \"ثم\", \"لكن\", \"أو\", \"كان\", \"كانت\", \"يكون\", \"وكان\", \"وكانت\", \"هو\", \"هي\",\n",
        "    \"كل\", \"بعض\", \"تم\", \"لا\", \"لم\", \"لن\", \"ما\", \"هناك\", \"كما\", \"قال\", \"قالت\", \"يقول\",\n",
        "    \"عندما\", \"عند\", \"حيث\", \"منذ\", \"خلال\", \"بعد\", \"قبل\", \"حتى\", \"اذا\", \"إذا\", \"بين\",\n",
        "    \"منها\", \"منه\", \"له\", \"لها\", \"التي\", \"الذي\", \"فيه\", \"فيها\", \"عنه\", \"عنها\"\n",
        "]\n",
        "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", stopWords=arabic_stopwords)\n",
        "filtered_data = stopwords_remover.transform(tokenized_data)\n",
        "\n",
        "# 2.5 Apply stemming with a simple UDF approach\n",
        "def simple_arabic_stem(word_list):\n",
        "    \"\"\"Simple Arabic stemmer that removes common prefixes and suffixes\"\"\"\n",
        "    if word_list is None:\n",
        "        return []\n",
        "\n",
        "    result = []\n",
        "\n",
        "    for word in word_list:\n",
        "        if len(word) <= 3:  # Don't stem short words\n",
        "            result.append(word)\n",
        "            continue\n",
        "\n",
        "        # Remove prefixes\n",
        "        if word.startswith(\"ال\") and len(word) > 4:\n",
        "            word = word[2:]\n",
        "        elif word.startswith((\"و\", \"ف\", \"ب\", \"ي\", \"ت\")) and len(word) > 3:\n",
        "            word = word[1:]\n",
        "\n",
        "        # Remove suffixes\n",
        "        if len(word) > 4:\n",
        "            if word.endswith((\"ون\", \"ين\", \"ات\", \"ان\")):\n",
        "                word = word[:-2]\n",
        "            elif word.endswith((\"تي\", \"يه\", \"ية\", \"ته\", \"ها\", \"هم\", \"هن\", \"نا\")):\n",
        "                word = word[:-2]\n",
        "            elif word.endswith((\"ه\", \"ة\", \"ت\")):\n",
        "                word = word[:-1]\n",
        "\n",
        "        result.append(word)\n",
        "\n",
        "    return result\n",
        "\n",
        "stem_udf = udf(simple_arabic_stem, ArrayType(StringType()))\n",
        "stemmed_data = filtered_data.withColumn(\"stemmed\", stem_udf(col(\"filtered\")))\n",
        "\n",
        "# 3. Feature Extraction for LDA\n",
        "# Using CountVectorizer to create document-term matrix\n",
        "cv = CountVectorizer(inputCol=\"stemmed\", outputCol=\"features\", vocabSize=10000, minDF=5, minTF=2)\n",
        "cv_model = cv.fit(stemmed_data)\n",
        "vectorized_data = cv_model.transform(stemmed_data)\n",
        "\n",
        "# 4. Build and Train LDA Model\n",
        "# Setting the number of topics to 5 (for the five target categories)\n",
        "lda = LDA(k=5, maxIter=50, featuresCol=\"features\", seed=42, optimizer='em', learningDecay=0.5)\n",
        "lda_model = lda.fit(vectorized_data)\n",
        "\n",
        "# 5. Topic Extraction and Interpretation\n",
        "# Get vocabulary from CountVectorizer\n",
        "vocab = cv_model.vocabulary\n",
        "\n",
        "# Get top terms for each topic from LDA model\n",
        "print(\"Top terms for each topic:\")\n",
        "topics = lda_model.describeTopics(maxTermsPerTopic=20)\n",
        "topics_with_terms = topics.rdd.map(\n",
        "    lambda topic: (\n",
        "        topic.topic,\n",
        "        [vocab[idx] for idx in topic.termIndices]\n",
        "    )\n",
        ").toDF([\"topic\", \"top_terms\"])\n",
        "topics_with_terms.show(truncate=False)\n",
        "\n",
        "# 6. Automatic Topic Labeling\n",
        "# Define category keywords (expanded with more terms per category)\n",
        "category_keywords = {\n",
        "    \"Politics\": [\n",
        "        \"رئيس\", \"وزير\", \"حكومة\", \"انتخابات\", \"سياسة\", \"برلمان\", \"دولة\", \"قرار\", \"سفير\", \"خارجية\",\n",
        "        \"رئاسة\", \"مجلس\", \"سلطة\", \"دبلوماسية\", \"سياسي\", \"زيارة\", \"قمة\", \"اجتماع\", \"وفد\", \"سفارة\",\n",
        "        \"دستور\", \"تصريح\", \"عهد\", \"امير\", \"ملك\", \"سلطان\", \"امن\", \"رئاسي\", \"وزارة\", \"حزب\", \"نائب\"\n",
        "    ],\n",
        "    \"Economy\": [\n",
        "        \"اقتصاد\", \"سوق\", \"مال\", \"شركة\", \"بنك\", \"استثمار\", \"مليون\", \"دولار\", \"تجارة\", \"ميزانية\",\n",
        "        \"تمويل\", \"اسهم\", \"مصرف\", \"انتاج\", \"سعر\", \"اسعار\", \"صناعة\", \"نفط\", \"ثروة\", \"معدن\",\n",
        "        \"ذهب\", \"منجم\", \"مشروع\", \"تنمية\", \"سندات\", \"صندوق\", \"راس مال\", \"تضخم\", \"عملة\", \"بورصة\"\n",
        "    ],\n",
        "    \"Sports\": [\n",
        "        \"مباراة\", \"لاعب\", \"فريق\", \"كرة\", \"منتخب\", \"بطولة\", \"دوري\", \"هدف\", \"مدرب\", \"فوز\",\n",
        "        \"قدم\", \"سلة\", \"يد\", \"طائرة\", \"سباق\", \"ملعب\", \"استاد\", \"تدريب\", \"مونديال\", \"اولمبياد\",\n",
        "        \"كاس\", \"جولة\", \"رياضي\", \"نادي\", \"مهاجم\", \"مدافع\", \"حارس\", \"مرمى\", \"تسجيل\", \"احراز\"\n",
        "    ],\n",
        "    \"Disasters\": [\n",
        "        \"امطار\", \"سيول\", \"فيضان\", \"عاصفة\", \"اعصار\", \"جفاف\", \"حريق\", \"كارثة\", \"ضحايا\", \"سد\",\n",
        "        \"انهيار\", \"غرق\", \"طوفان\", \"تضرر\", \"خسائر\", \"منكوبة\", \"اضرار\", \"انقاذ\", \"اغاثة\", \"طوارئ\",\n",
        "        \"مياه\", \"محاصر\", \"حادث\", \"وفاة\", \"مصاب\", \"اصابة\", \"كسر\", \"تحطم\", \"مطر\", \"رياح\"\n",
        "    ],\n",
        "    \"Social\": [\n",
        "        \"مجتمع\", \"صحة\", \"تعليم\", \"مدرسة\", \"طلاب\", \"مستشفى\", \"مرض\", \"دواء\", \"جامعة\", \"عائلة\",\n",
        "        \"طفل\", \"اطفال\", \"نساء\", \"رجال\", \"شباب\", \"كبار\", \"خدمات\", \"تطوع\", \"حملة\", \"توعية\",\n",
        "        \"فعالية\", \"مبادرة\", \"مهرجان\", \"احتفال\", \"منظمة\", \"جمعية\", \"هيئة\", \"مؤسسة\", \"عامة\", \"ثقافة\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# UDF to assign category based on keyword overlap with topic terms\n",
        "def assign_category(top_terms):\n",
        "    if not top_terms:\n",
        "        return \"Unknown\"\n",
        "\n",
        "    scores = {}\n",
        "    for category, keywords in category_keywords.items():\n",
        "        # Count the number of topic terms that appear in each category's keywords\n",
        "        score = sum(1 for term in top_terms if term in keywords)\n",
        "        scores[category] = score\n",
        "\n",
        "    # Return the category with the highest score\n",
        "    if max(scores.values()) > 0:\n",
        "        return max(scores.items(), key=lambda x: x[1])[0]\n",
        "    else:\n",
        "        return \"Unknown\"\n",
        "\n",
        "# Register UDF\n",
        "assign_category_udf = udf(assign_category, StringType())\n",
        "\n",
        "# Apply UDF to get category labels for each topic\n",
        "labeled_topics = topics_with_terms.withColumn(\"category\", assign_category_udf(col(\"top_terms\")))\n",
        "print(\"Topics with assigned categories:\")\n",
        "labeled_topics.show(truncate=False)\n",
        "\n",
        "# Create mapping dictionary from topic ID to category\n",
        "topic_to_category_df = labeled_topics.select(\"topic\", \"category\")\n",
        "topic_to_category = {row[\"topic\"]: row[\"category\"] for row in topic_to_category_df.collect()}\n",
        "print(\"Topic to Category Mapping:\", topic_to_category)\n",
        "\n",
        "# 7. Transform the dataset to get topic distributions for all documents\n",
        "topics_data = lda_model.transform(vectorized_data)\n",
        "\n",
        "# 8. Function to extract dominant topic\n",
        "def get_dominant_topic(topic_distribution):\n",
        "    \"\"\"Get the index of the topic with highest probability\"\"\"\n",
        "    return int(np.argmax(topic_distribution))\n",
        "\n",
        "# Register UDF\n",
        "get_dominant_topic_udf = udf(get_dominant_topic, StringType())\n",
        "\n",
        "# Get dominant topic for each document\n",
        "categorized_data = topics_data.withColumn(\"dominant_topic\", get_dominant_topic_udf(col(\"topicDistribution\")))\n",
        "\n",
        "# 9. Map dominant topic to category\n",
        "def map_topic_to_category(topic_id):\n",
        "    \"\"\"Map topic ID to the assigned category\"\"\"\n",
        "    return topic_to_category.get(int(topic_id), \"Unknown\")\n",
        "\n",
        "# Register UDF\n",
        "map_topic_udf = udf(map_topic_to_category, StringType())\n",
        "\n",
        "# Apply the mapping to get final category for each document\n",
        "final_data = categorized_data.withColumn(\"category\", map_topic_udf(col(\"dominant_topic\")))\n",
        "\n",
        "# 10. Show results\n",
        "print(\"Sample of categorized articles:\")\n",
        "final_data.select(\"id\", \"title\", \"dominant_topic\", \"category\").show(10, truncate=80)\n",
        "\n",
        "# 11. Evaluate the distribution of articles by category\n",
        "category_distribution = final_data.groupBy(\"category\").count().orderBy(\"count\", ascending=False)\n",
        "print(\"Distribution of articles by category:\")\n",
        "category_distribution.show()\n",
        "\n",
        "# 12. Demonstration on two most recent articles\n",
        "# Convert pub_date to timestamp and select the two most recent articles\n",
        "from pyspark.sql.functions import to_timestamp\n",
        "data_with_timestamp = data.withColumn(\"timestamp\", to_timestamp(col(\"pub_date\")))\n",
        "recent_articles = data_with_timestamp.orderBy(col(\"timestamp\").desc()).limit(2)\n",
        "\n",
        "# Use the existing pipeline stages to process these articles\n",
        "recent_tokenized = tokenizer.transform(recent_articles)\n",
        "recent_filtered = stopwords_remover.transform(recent_tokenized)\n",
        "recent_stemmed = recent_filtered.withColumn(\"stemmed\", stem_udf(col(\"filtered\")))\n",
        "recent_vectorized = cv_model.transform(recent_stemmed)\n",
        "recent_topics = lda_model.transform(recent_vectorized)\n",
        "\n",
        "# Get dominant topic and category\n",
        "recent_categorized = recent_topics \\\n",
        "    .withColumn(\"dominant_topic\", get_dominant_topic_udf(col(\"topicDistribution\"))) \\\n",
        "    .withColumn(\"category\", map_topic_udf(col(\"dominant_topic\")))\n",
        "\n",
        "# Show the results for the recent articles\n",
        "print(\"Categorization of the two most recent articles:\")\n",
        "recent_categorized.select(\"id\", \"title\", \"dominant_topic\", \"category\", \"pub_date\").show(truncate=80)\n",
        "\n",
        "# Save the models for future use\n",
        "lda_model.save(\"arabic_news_lda_model\")\n",
        "cv_model.save(\"arabic_news_cv_model\")\n",
        "print(\"Models saved successfully.\")\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ylF-RwCfwmk",
        "outputId": "3745819e-039e-44d0-8500-5b89f404d4e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Schema:\n",
            "root\n",
            " |-- content: string (nullable = true)\n",
            " |-- description: string (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- link: string (nullable = true)\n",
            " |-- pub_date: string (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            "\n",
            "Sample Data:\n",
            "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+-----+-----------------------------------+-------------------------+------------------------------------------------------------------------+\n",
            "|                                                                         content|                                                                     description|   id|                               link|                 pub_date|                                                                   title|\n",
            "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+-----+-----------------------------------+-------------------------+------------------------------------------------------------------------+\n",
            "|لأخبار (نواكشوط) استأنف  قطار الشركة الوطنية للصناعة والمناجم اسنيم قبل أقل م...|لأخبار (نواكشوط) استأنف قطار الشركة الوطنية للصناعة والمناجم اسنيم قبل أقل من...|41129|https://alakhbar.info/?q=node/41129|2022-06-21T00:09:59+00:00|                              بعد إصلاح عطب بالسكك قطار اسنيم يعود للعمل|\n",
            "|الأخبار (نواكشوط) ـ أعلنت اللجنة الفنية المكلفة بمتابعة أعمال اللجنة الوزارية...|الأخبار (نواكشوط) ـ أعلنت اللجنة الفنية المكلفة بمتابعة أعمال اللجنة الوزارية...|41131|https://alakhbar.info/?q=node/41131|2022-06-21T09:13:54+00:00|                 أضرار في الطرق والمساكن بخمس ولايات إثر الأمطار والسيول|\n",
            "|الأخبار (وادان) – عزلت السيول التي تدفقت صباح اليوم من المرتفعات الجبلية مدين...|الأخبار (وادان) – عزلت السيول التي تدفقت صباح اليوم من المرتفعات الجبلية مدين...|41132|https://alakhbar.info/?q=node/41132|2022-06-21T09:51:03+00:00|                  السيول تعزل مدينة ودان وتوقف تنقل السكان منها أو إليها|\n",
            "|الأخبار (وادان) – وصف رئيس حي \"كَافي\" بضاحية مدينة ودان التاريخية اسلاكه ولد ...|الأخبار (وادان) – وصف رئيس حي \"كَافي\" بضاحية مدينة ودان التاريخية اسلاكه ولد ...|41133|https://alakhbar.info/?q=node/41133|2022-06-21T09:54:31+00:00|رئيس حي \"كَافي\" بوادان: نحن محاصرون وهناك أسر منكوبة بسبب السيول (فيديو)|\n",
            "|الأخبار (وادان) – عزلت السيول التي تدفقت صباح اليوم من المرتفعات الجبلية مدين...|الأخبار (وادان) – عزلت السيول التي تدفقت صباح اليوم من المرتفعات الجبلية مدين...|41134|https://alakhbar.info/?q=node/41134|2022-06-21T10:57:00+00:00|          السيول تعزل مدينة ودان وتوقف تنقل السكان منها أو إليها (فيديو)|\n",
            "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+-----+-----------------------------------+-------------------------+------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Top terms for each topic:\n",
            "+-----+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "|topic|top_terms                                                                                                                          |\n",
            "+-----+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "|0    |[علي, الي, ان, موريتانيا, رئيس, دول, اخبار, مالي, مجال, اقتصاد, زير, مجموع, بلد, خارج, شركة, نواكشوط, حكوم, افريقيا, قطاع, موريتان]|\n",
            "|1    |[ولد, محمد, رئيس, علي, ان, عبد, احمد, سابق, مدير, شيخ, عام, قان, ادار, مجلس, عامة, عزيز, انه, محكم, الي, سيدي]                     |\n",
            "|2    |[نواكشوط, اخبار, يوم, الي, مم, مدين, علي, نواذيبو, عاصم, لاية, اعلن, مقاطع, زارة, وزار, اوق, ان, وقت, جديد, مساء, يما]             |\n",
            "|3    |[ان, علي, الي, او, ذلك, الله, انه, غير, ولا, ملم, دولة, اي, عليه, وهو, جميع, حتي, دون, الا, وفي, وقد]                              |\n",
            "|4    |[نواكشوط, اخبار, علي, ولد, ان, حزب, غزواني, وطني, رئيس, انتخاب, الي, وطن, عمل, يوم, اجتماع, لجنة, اضاف, تعليم, موريتاني, حكوم]     |\n",
            "+-----+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "Topics with assigned categories:\n",
            "+-----+-----------------------------------------------------------------------------------------------------------------------------------+--------+\n",
            "|topic|top_terms                                                                                                                          |category|\n",
            "+-----+-----------------------------------------------------------------------------------------------------------------------------------+--------+\n",
            "|0    |[علي, الي, ان, موريتانيا, رئيس, دول, اخبار, مالي, مجال, اقتصاد, زير, مجموع, بلد, خارج, شركة, نواكشوط, حكوم, افريقيا, قطاع, موريتان]|Economy |\n",
            "|1    |[ولد, محمد, رئيس, علي, ان, عبد, احمد, سابق, مدير, شيخ, عام, قان, ادار, مجلس, عامة, عزيز, انه, محكم, الي, سيدي]                     |Politics|\n",
            "|2    |[نواكشوط, اخبار, يوم, الي, مم, مدين, علي, نواذيبو, عاصم, لاية, اعلن, مقاطع, زارة, وزار, اوق, ان, وقت, جديد, مساء, يما]             |Unknown |\n",
            "|3    |[ان, علي, الي, او, ذلك, الله, انه, غير, ولا, ملم, دولة, اي, عليه, وهو, جميع, حتي, دون, الا, وفي, وقد]                              |Politics|\n",
            "|4    |[نواكشوط, اخبار, علي, ولد, ان, حزب, غزواني, وطني, رئيس, انتخاب, الي, وطن, عمل, يوم, اجتماع, لجنة, اضاف, تعليم, موريتاني, حكوم]     |Politics|\n",
            "+-----+-----------------------------------------------------------------------------------------------------------------------------------+--------+\n",
            "\n",
            "Topic to Category Mapping: {0: 'Economy', 1: 'Politics', 2: 'Unknown', 3: 'Politics', 4: 'Politics'}\n",
            "Sample of categorized articles:\n",
            "+-----+------------------------------------------------------------------------+--------------+--------+\n",
            "|   id|                                                                   title|dominant_topic|category|\n",
            "+-----+------------------------------------------------------------------------+--------------+--------+\n",
            "|41129|                              بعد إصلاح عطب بالسكك قطار اسنيم يعود للعمل|             2| Unknown|\n",
            "|41131|                 أضرار في الطرق والمساكن بخمس ولايات إثر الأمطار والسيول|             2| Unknown|\n",
            "|41132|                  السيول تعزل مدينة ودان وتوقف تنقل السكان منها أو إليها|             2| Unknown|\n",
            "|41133|رئيس حي \"كَافي\" بوادان: نحن محاصرون وهناك أسر منكوبة بسبب السيول (فيديو)|             2| Unknown|\n",
            "|41134|          السيول تعزل مدينة ودان وتوقف تنقل السكان منها أو إليها (فيديو)|             2| Unknown|\n",
            "|41135|                  منتخبو وادان: السلطات تسرعت في تقييمها لحجم الأضرار   |             2| Unknown|\n",
            "|41136|           نقابيون وبحارة يحتجون على إعفاء مدير الدائرة البحرية بنواذيبو|             1|Politics|\n",
            "|41137|          مالي تبلغ مواطنيها رفع اشتراط رخصة الدخول للعبور إلى موريتانيا|             0| Economy|\n",
            "|41138|                 غزواني: نتمتع بموقع استراتيجي ممتاز ومنفتحون على أوروبا|             0| Economy|\n",
            "|41139|                     الهابا تطلق دورة تكوينية حول الصحافة والتحدي الرقمي|             4|Politics|\n",
            "+-----+------------------------------------------------------------------------+--------------+--------+\n",
            "only showing top 10 rows\n",
            "\n",
            "Distribution of articles by category:\n",
            "+--------+-----+\n",
            "|category|count|\n",
            "+--------+-----+\n",
            "|Politics|15730|\n",
            "| Economy| 8343|\n",
            "| Unknown| 5579|\n",
            "+--------+-----+\n",
            "\n",
            "Categorization of the two most recent articles:\n",
            "+-----+-----------------------------------------------------------+--------------+--------+-------------------------+\n",
            "|   id|                                                      title|dominant_topic|category|                 pub_date|\n",
            "+-----+-----------------------------------------------------------+--------------+--------+-------------------------+\n",
            "|58999|وزير الدفاع يغادر إلى التشيك ضمن جولة خارجية تستمر 8 أيام  |             0| Economy|2025-02-11T14:18:53+00:00|\n",
            "|58998| موريتانيا والكويت تبحثان التعاون في إدارة المخاطر والكوارث|             0| Economy|2025-02-11T12:06:13+00:00|\n",
            "+-----+-----------------------------------------------------------+--------------+--------+-------------------------+\n",
            "\n",
            "Models saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Any\n",
        "import re\n",
        "import numpy as np\n",
        "import unittest\n",
        "\n",
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from pyspark.sql.functions import concat_ws, col, udf, to_timestamp\n",
        "from pyspark.sql.types import StringType, ArrayType\n",
        "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
        "from pyspark.ml.clustering import LDA\n",
        "\n",
        "# --- Module-level UDF helpers ---\n",
        "\n",
        "def normalize_arabic(text: str) -> str:\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    text = (text.replace(\"أ\", \"ا\")\n",
        "                .replace(\"إ\", \"ا\")\n",
        "                .replace(\"آ\", \"ا\")\n",
        "                .replace(\"ى\", \"ي\"))\n",
        "    text = re.sub(r'[\\u064B-\\u065F\\u0640]', '', text)\n",
        "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', ' ', text)\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "normalize_udf = udf(normalize_arabic, StringType())\n",
        "\n",
        "def simple_arabic_stem(word_list: List[str]) -> List[str]:\n",
        "    if not word_list:\n",
        "        return []\n",
        "    result = []\n",
        "    for word in word_list:\n",
        "        if len(word) <= 3:\n",
        "            result.append(word)\n",
        "            continue\n",
        "        if word.startswith(\"ال\") and len(word) > 4:\n",
        "            word = word[2:]\n",
        "        elif word[0] in (\"و\",\"ف\",\"ب\",\"ي\",\"ت\") and len(word) > 3:\n",
        "            word = word[1:]\n",
        "        if len(word) > 4:\n",
        "            if word.endswith((\"ون\",\"ين\",\"ات\",\"ان\")):\n",
        "                word = word[:-2]\n",
        "            elif word.endswith((\"تي\",\"يه\",\"ية\",\"ته\",\"ها\",\"هم\",\"هن\",\"نا\")):\n",
        "                word = word[:-2]\n",
        "            elif word.endswith((\"ه\",\"ة\",\"ت\")):\n",
        "                word = word[:-1]\n",
        "        result.append(word)\n",
        "    return result\n",
        "\n",
        "stem_udf = udf(simple_arabic_stem, ArrayType(StringType()))\n",
        "\n",
        "def assign_category_udf_factory(category_keywords: Dict[str,List[str]]):\n",
        "    def assign_category(top_terms: List[str]) -> str:\n",
        "        if not top_terms:\n",
        "            return \"Unknown\"\n",
        "        scores = {\n",
        "            cat: sum(1 for term in top_terms if term in kw)\n",
        "            for cat, kw in category_keywords.items()\n",
        "        }\n",
        "        best, best_score = max(scores.items(), key=lambda x: x[1])\n",
        "        return best if best_score > 0 else \"Unknown\"\n",
        "    return udf(assign_category, StringType())\n",
        "\n",
        "def get_dominant_topic(dist: List[float]) -> str:\n",
        "    return str(int(np.argmax(dist)))\n",
        "\n",
        "get_dominant_topic_udf = udf(get_dominant_topic, StringType())\n",
        "\n",
        "# --- Main class ---\n",
        "\n",
        "class ArabicNewsTopicModeler:\n",
        "    def __init__(self, app_name: str = \"ArabicNewsTopicModeling\", memory: str = \"4g\"):\n",
        "        self.spark = SparkSession.builder \\\n",
        "            .appName(app_name) \\\n",
        "            .config(\"spark.driver.memory\", memory) \\\n",
        "            .getOrCreate()\n",
        "        self.category_keywords = {\n",
        "            \"Politics\": [\"رئيس\",\"وزير\",\"حكومة\",\"انتخابات\",\"سياسة\",\"برلمان\",\"دولة\",\"قرار\",\"سفير\",\"خارجية\",\"رئاسة\",\"مجلس\",\"سلطة\"],\n",
        "            \"Economy\":  [\"اقتصاد\",\"سوق\",\"مال\",\"شركة\",\"بنك\",\"استثمار\",\"مليون\",\"دولار\",\"تجارة\",\"ميزانية\",\"تمويل\",\"اسهم\",\"مصرف\"],\n",
        "            \"Sports\":   [\"مباراة\",\"لاعب\",\"فريق\",\"كرة\",\"منتخب\",\"بطولة\",\"دوري\",\"هدف\",\"مدرب\",\"فوز\",\"قدم\",\"سلة\",\"يد\"],\n",
        "            \"Disasters\":[\"امطار\",\"سيول\",\"فيضان\",\"عاصفة\",\"اعصار\",\"جفاف\",\"حريق\",\"كارثة\",\"ضحايا\",\"سد\",\"انهيار\",\"غرق\",\"طوفان\"],\n",
        "            \"Social\":   [\"مجتمع\",\"صحة\",\"تعليم\",\"مدرسة\",\"طلاب\",\"مستشفى\",\"مرض\",\"دواء\",\"جامعة\",\"عائلة\",\"طفل\",\"اطفال\",\"نساء\"]\n",
        "        }\n",
        "        self.assign_category_udf = assign_category_udf_factory(self.category_keywords)\n",
        "\n",
        "    def preprocess_data(self, input_path: str) -> DataFrame:\n",
        "        df = self.spark.read.json(input_path)\n",
        "        df = df.withColumn(\"text\", concat_ws(\" \", col(\"title\"), col(\"description\"), col(\"content\")))\n",
        "        return df.withColumn(\"norm_text\", normalize_udf(col(\"text\")))\n",
        "\n",
        "    def tokenize_and_filter(self, df: DataFrame) -> DataFrame:\n",
        "        tokenizer = RegexTokenizer(inputCol=\"norm_text\", outputCol=\"words\", pattern=\"[^\\\\p{L}\\\\p{N}_]+\")\n",
        "        df = tokenizer.transform(df)\n",
        "        stopwords = [\"في\",\"من\",\"على\",\"إلى\",\"عن\",\"مع\",\"هذا\",\"هذه\",\"أن\",\"و\",\"ب\",\"ل\",\"الى\",\"إن\",\"ثم\",\"لكن\",\"أو\",\"كان\",\"كانت\"]\n",
        "        remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", stopWords=stopwords)\n",
        "        df = remover.transform(df)\n",
        "        return df.withColumn(\"stemmed\", stem_udf(col(\"filtered\")))\n",
        "\n",
        "    def extract_topics(self, df: DataFrame, num_topics: int = 5):\n",
        "        cv = CountVectorizer(inputCol=\"stemmed\", outputCol=\"features\", vocabSize=10000, minDF=5, minTF=2)\n",
        "        cv_model = cv.fit(df)\n",
        "        vect = cv_model.transform(df)\n",
        "        lda = LDA(k=num_topics, maxIter=50, featuresCol=\"features\", seed=42, optimizer='em', learningDecay=0.5)\n",
        "        lda_model = lda.fit(vect)\n",
        "        return lda_model, cv_model, vect\n",
        "\n",
        "    def label_topics(self, lda_model, cv_model) -> DataFrame:\n",
        "        vocab = cv_model.vocabulary\n",
        "        topics = lda_model.describeTopics(maxTermsPerTopic=20).collect()\n",
        "        rows = []\n",
        "        for t in topics:\n",
        "            terms = [vocab[idx] for idx in t.termIndices]\n",
        "            rows.append((t.topic, terms))\n",
        "        topics_df = self.spark.createDataFrame(rows, [\"topic\",\"top_terms\"])\n",
        "        return topics_df.withColumn(\"category\", self.assign_category_udf(col(\"top_terms\")))\n",
        "\n",
        "    def categorize_documents(self, df_with_dist: DataFrame, topic_to_cat: Dict[int,str]) -> DataFrame:\n",
        "        df2 = df_with_dist.withColumn(\"dominant_topic\", get_dominant_topic_udf(col(\"topicDistribution\")))\n",
        "        def map_topic(tid): return topic_to_cat.get(int(tid), \"Unknown\")\n",
        "        map_udf = udf(map_topic, StringType())\n",
        "        return df2.withColumn(\"category\", map_udf(col(\"dominant_topic\")))\n",
        "\n",
        "    # ... (Other methods like run_topic_modeling, tests, etc., remain unchanged) ...\n",
        "\n",
        "\n",
        "    def run_topic_modeling(self, input_path: str) -> None:\n",
        "        \"\"\"\n",
        "        Full pipeline for Arabic news topic modeling\n",
        "\n",
        "        :param input_path: Path to input JSON file\n",
        "        \"\"\"\n",
        "        # Preprocess data\n",
        "        preprocessed_data = self.preprocess_data(input_path)\n",
        "\n",
        "        # Tokenize and filter\n",
        "        stemmed_data = self.tokenize_and_filter(preprocessed_data)\n",
        "\n",
        "        # Extract topics\n",
        "        lda_model, cv_model, vectorized_data = self.extract_topics(stemmed_data)\n",
        "\n",
        "        # Label topics\n",
        "        labeled_topics = self.label_topics(lda_model, cv_model)\n",
        "        labeled_topics.show(truncate=False)\n",
        "\n",
        "        # Create topic to category mapping\n",
        "        topic_to_category = {\n",
        "            row[\"topic\"]: row[\"category\"]\n",
        "            for row in labeled_topics.collect()\n",
        "        }\n",
        "        print(\"Topic to Category Mapping:\", topic_to_category)\n",
        "\n",
        "        # Categorize documents\n",
        "        final_data = self.categorize_documents(\n",
        "            lda_model.transform(vectorized_data),\n",
        "            lda_model,\n",
        "            topic_to_category\n",
        "        )\n",
        "\n",
        "        # Show results\n",
        "        print(\"Sample of categorized articles:\")\n",
        "        final_data.select(\"id\", \"title\", \"dominant_topic\", \"category\").show(10, truncate=80)\n",
        "\n",
        "        # Analyze category distribution\n",
        "        category_distribution = final_data.groupBy(\"category\").count().orderBy(\"count\", ascending=False)\n",
        "        print(\"Distribution of articles by category:\")\n",
        "        category_distribution.show()\n",
        "\n",
        "        # Optional: Demonstrate on recent articles\n",
        "        self._analyze_recent_articles(\n",
        "            preprocessed_data,\n",
        "            lda_model,\n",
        "            cv_model,\n",
        "            topic_to_category\n",
        "        )\n",
        "\n",
        "        # Save models\n",
        "        lda_model.save(\"arabic_news_lda_model\")\n",
        "        cv_model.save(\"arabic_news_cv_model\")\n",
        "        print(\"Models saved successfully.\")\n",
        "\n",
        "    def _analyze_recent_articles(self,\n",
        "                                 data: DataFrame,\n",
        "                                 lda_model: Any,\n",
        "                                 cv_model: Any,\n",
        "                                 topic_to_category: Dict[int, str]) -> None:\n",
        "        \"\"\"\n",
        "        Analyze the two most recent articles\n",
        "\n",
        "        :param data: Original DataFrame\n",
        "        :param lda_model: Trained LDA model\n",
        "        :param cv_model: Trained CountVectorizer model\n",
        "        :param topic_to_category: Mapping of topic IDs to categories\n",
        "        \"\"\"\n",
        "        # Add timestamp column and select recent articles\n",
        "        data_with_timestamp = data.withColumn(\"timestamp\", to_timestamp(col(\"pub_date\")))\n",
        "        recent_articles = data_with_timestamp.orderBy(col(\"timestamp\").desc()).limit(2)\n",
        "\n",
        "        # Reuse the existing preprocessing pipeline\n",
        "        recent_tokenized = self.tokenize_and_filter(recent_articles)\n",
        "        recent_vectorized = cv_model.transform(recent_tokenized)\n",
        "        recent_topics = lda_model.transform(recent_vectorized)\n",
        "\n",
        "        # Categorize recent articles\n",
        "        recent_categorized = self.categorize_documents(\n",
        "            recent_topics,\n",
        "            lda_model,\n",
        "            topic_to_category\n",
        "        )\n",
        "\n",
        "        # Show results for recent articles\n",
        "        print(\"Categorization of the two most recent articles:\")\n",
        "        recent_categorized.select(\"id\", \"title\", \"dominant_topic\", \"category\", \"pub_date\").show(truncate=80)\n",
        "\n",
        "class ArabicNewsTopicModelingTest(unittest.TestCase):\n",
        "    \"\"\"\n",
        "    Test suite for Arabic News Topic Modeling Pipeline\n",
        "    \"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def setUpClass(cls):\n",
        "        \"\"\"\n",
        "        Set up resources for the entire test suite\n",
        "        \"\"\"\n",
        "        cls.modeler = ArabicNewsTopicModeler()\n",
        "\n",
        "        # Prepare a small sample dataset for testing\n",
        "        sample_data = [\n",
        "            {\n",
        "                \"id\": \"1\",\n",
        "                \"title\": \"سياسة الاقتصاد في المنطقة\",\n",
        "                \"description\": \"تحليل شامل للوضع الاقتصادي\",\n",
        "                \"content\": \"في ظل التغيرات السياسية الحالية، يواجه الاقتصاد تحديات كبيرة\",\n",
        "                \"pub_date\": \"2024-01-15\"\n",
        "            },\n",
        "            {\n",
        "                \"id\": \"2\",\n",
        "                \"title\": \"منتخب كرة القدم يحقق فوزًا تاريخيًا\",\n",
        "                \"description\": \"فوز مثير في تصفيات كأس العالم\",\n",
        "                \"content\": \"حقق منتخبنا الوطني فوزًا مهمًا على المنتخب المنافس\",\n",
        "                \"pub_date\": \"2024-01-16\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Save sample data to a temporary file for testing\n",
        "        cls.temp_json_path = \"test_arabic_news.json\"\n",
        "        cls.spark = ArabicNewsTopicModeler().spark\n",
        "        df = cls.spark.createDataFrame(sample_data)\n",
        "        df.write.json(cls.temp_json_path, mode=\"overwrite\")\n",
        "\n",
        "    @classmethod\n",
        "    def tearDownClass(cls):\n",
        "        \"\"\"\n",
        "        Clean up resources after testing\n",
        "        \"\"\"\n",
        "        # Remove temporary test file\n",
        "        import shutil\n",
        "        try:\n",
        "            shutil.rmtree(cls.temp_json_path)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # Stop Spark session\n",
        "        cls.modeler.spark.stop()\n",
        "\n",
        "    def test_text_normalization(self):\n",
        "        \"\"\"\n",
        "        Test Arabic text normalization method\n",
        "        \"\"\"\n",
        "        test_cases = [\n",
        "            (\"أحمد\", \"احمد\"),  # Test Alef variations\n",
        "            (\"مُحَمَّد\", \"محمد\"),  # Test diacritics removal\n",
        "            (\"سَلَام\", \"سلام\"),  # Test various diacritics\n",
        "        ]\n",
        "\n",
        "        for input_text, expected in test_cases:\n",
        "            normalized = self.modeler._normalize_arabic(input_text)\n",
        "            self.assertEqual(normalized, expected,\n",
        "                             f\"Failed to normalize: {input_text}\")\n",
        "\n",
        "    def test_arabic_stemming(self):\n",
        "        \"\"\"\n",
        "        Test Arabic simple stemming method\n",
        "        \"\"\"\n",
        "        test_cases = [\n",
        "            ([\"المدرسة\"], [\"درسة\"]),  # Test prefix removal\n",
        "            ([\"يكتبون\"], [\"كتب\"]),  # Test suffix removal\n",
        "            ([\"كتاب\"], [\"كتاب\"]),  # Test short word preservation\n",
        "        ]\n",
        "\n",
        "        for input_words, expected in test_cases:\n",
        "            stemmed = self.modeler._simple_arabic_stem(input_words)\n",
        "            self.assertEqual(stemmed, expected,\n",
        "                             f\"Failed to stem: {input_words}\")\n",
        "\n",
        "    def test_category_assignment(self):\n",
        "        \"\"\"\n",
        "        Test topic category assignment\n",
        "        \"\"\"\n",
        "        test_cases = [\n",
        "            ([\"رئيس\", \"وزير\", \"حكومة\"], \"Politics\"),\n",
        "            ([\"مباراة\", \"لاعب\", \"كرة\"], \"Sports\"),\n",
        "            ([\"اقتصاد\", \"سوق\", \"مال\"], \"Economy\"),\n",
        "            ([], \"Unknown\")\n",
        "        ]\n",
        "\n",
        "        for terms, expected_category in test_cases:\n",
        "            category = self.modeler._assign_category(terms)\n",
        "            self.assertEqual(category, expected_category,\n",
        "                             f\"Failed to categorize: {terms}\")\n",
        "\n",
        "    def test_pipeline_end_to_end(self):\n",
        "        \"\"\"\n",
        "        Test the full topic modeling pipeline\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Preprocess data\n",
        "            preprocessed_data = self.modeler.preprocess_data(self.temp_json_path)\n",
        "\n",
        "            # Tokenize and filter\n",
        "            stemmed_data = self.modeler.tokenize_and_filter(preprocessed_data)\n",
        "\n",
        "            # Extract topics\n",
        "            lda_model, cv_model, vectorized_data = self.modeler.extract_topics(stemmed_data)\n",
        "\n",
        "            # Label topics\n",
        "            labeled_topics = self.modeler.label_topics(lda_model, cv_model)\n",
        "\n",
        "            # Create topic to category mapping\n",
        "            topic_to_category = {\n",
        "                row[\"topic\"]: row[\"category\"]\n",
        "                for row in labeled_topics.collect()\n",
        "            }\n",
        "\n",
        "            # Categorize documents\n",
        "            final_data = self.modeler.categorize_documents(\n",
        "                lda_model.transform(vectorized_data),\n",
        "                lda_model,\n",
        "                topic_to_category\n",
        "            )\n",
        "\n",
        "            # Assertions\n",
        "            self.assertIsNotNone(final_data)\n",
        "            self.assertTrue(final_data.count() > 0)\n",
        "\n",
        "            # Check if categories are assigned\n",
        "            categories = final_data.select(\"category\").distinct().collect()\n",
        "            self.assertTrue(len(categories) > 0)\n",
        "\n",
        "        except Exception as e:\n",
        "            self.fail(f\"Pipeline test failed: {e}\")\n",
        "\n",
        "def run_pipeline_validation(input_path: str):\n",
        "    \"\"\"\n",
        "    Validate the topic modeling pipeline with detailed logging\n",
        "\n",
        "    :param input_path: Path to the input JSON file\n",
        "    \"\"\"\n",
        "    print(\"Starting Pipeline Validation...\")\n",
        "\n",
        "    # Initialize the topic modeler\n",
        "    modeler = ArabicNewsTopicModeler()\n",
        "\n",
        "    try:\n",
        "        # Preprocess data\n",
        "        print(\"1. Preprocessing Data...\")\n",
        "        preprocessed_data = modeler.preprocess_data(input_path)\n",
        "        print(f\"   Total documents: {preprocessed_data.count()}\")\n",
        "\n",
        "        # Tokenize and filter\n",
        "        print(\"2. Tokenizing and Filtering...\")\n",
        "        stemmed_data = modeler.tokenize_and_filter(preprocessed_data)\n",
        "        print(f\"   Processed documents: {stemmed_data.count()}\")\n",
        "\n",
        "        # Extract topics\n",
        "        print(\"3. Extracting Topics...\")\n",
        "        lda_model, cv_model, vectorized_data = modeler.extract_topics(stemmed_data)\n",
        "\n",
        "        # Label topics\n",
        "        print(\"4. Labeling Topics...\")\n",
        "        labeled_topics = modeler.label_topics(lda_model, cv_model)\n",
        "        print(\"   Topic Labels:\")\n",
        "        labeled_topics.show(truncate=False)\n",
        "\n",
        "        # Create topic to category mapping\n",
        "        topic_to_category = {\n",
        "            row[\"topic\"]: row[\"category\"]\n",
        "            for row in labeled_topics.collect()\n",
        "        }\n",
        "        print(\"   Topic to Category Mapping:\", topic_to_category)\n",
        "\n",
        "        # Categorize documents\n",
        "        print(\"5. Categorizing Documents...\")\n",
        "        final_data = modeler.categorize_documents(\n",
        "            lda_model.transform(vectorized_data),\n",
        "            lda_model,\n",
        "            topic_to_category\n",
        "        )\n",
        "\n",
        "        # Show category distribution\n",
        "        print(\"6. Category Distribution:\")\n",
        "        category_distribution = final_data.groupBy(\"category\").count().orderBy(\"count\", ascending=False)\n",
        "        category_distribution.show()\n",
        "\n",
        "        print(\"Pipeline Validation Completed Successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Pipeline Validation Failed: {e}\")\n",
        "\n",
        "    finally:\n",
        "        # Stop Spark session\n",
        "        modeler.spark.stop()\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to demonstrate the Arabic News Topic Modeling pipeline\n",
        "    \"\"\"\n",
        "    # Uncomment and modify the path as needed\n",
        "    input_path = \"alakhbar.json\"  # Replace with your actual input file path\n",
        "\n",
        "    # Option 1: Run unit tests\n",
        "    print(\"Running Unit Tests...\")\n",
        "    unittest.main(exit=False)\n",
        "\n",
        "    # Option 2: Validate pipeline with sample data\n",
        "    print(\"\\nValidating Pipeline...\")\n",
        "    run_pipeline_validation(input_path)\n",
        "\n",
        "    # Option 3: Full topic modeling\n",
        "    print(\"\\nRunning Full Topic Modeling...\")\n",
        "    modeler = ArabicNewsTopicModeler()\n",
        "    modeler.run_topic_modeling(input_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cFvTu3mLKzI_",
        "outputId": "7290cb6d-228f-4d82-cb5e-c0c45d87728d"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E\n",
            "======================================================================\n",
            "ERROR: /root/ (unittest.loader._FailedTest./root/)\n",
            "----------------------------------------------------------------------\n",
            "AttributeError: module '__main__' has no attribute '/root/'\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.031s\n",
            "\n",
            "FAILED (errors=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Unit Tests...\n",
            "\n",
            "Validating Pipeline...\n",
            "Starting Pipeline Validation...\n",
            "1. Preprocessing Data...\n",
            "   Total documents: 7800\n",
            "2. Tokenizing and Filtering...\n",
            "   Processed documents: 7800\n",
            "3. Extracting Topics...\n",
            "4. Labeling Topics...\n",
            "   Topic Labels:\n",
            "+-----+-------------------------------------------------------------------------------------------------------------------------+---------+\n",
            "|topic|top_terms                                                                                                                |category |\n",
            "+-----+-------------------------------------------------------------------------------------------------------------------------+---------+\n",
            "|0    |[اخبار, رئيس, نواكشوط, ان, يوم, مالي, علي, الي, اتحاد, قال, اعلن, خلال, يان, مجموع, التي, امن, لجنة, بلاد, دول, افريقيا] |Politics |\n",
            "|1    |[علي, ان, الي, مم, ما, لا, التي, الله, الذي, ذلك, او, كل, ولا, لم, ولد, حزب, انه, محمد, هو, بين]                         |Unknown  |\n",
            "|2    |[ان, علي, الي, ولد, التي, شركة, انه, ما, ادار, سابق, قان, او, تم, غير, الذي, عبد, ذلك, اوق, محمد, قديم]                  |Economy  |\n",
            "|3    |[ولد, محمد, علي, موريتانيا, نواكشوط, خلال, غزواني, رئيس, وزير, زير, عمل, الي, حكوم, عام, قطاع, ان, التي, احمد, مدير, بين]|Politics |\n",
            "|4    |[ملم, نواكشوط, اخبار, الي, علي, يوم, ان, مدين, نواذيبو, جديد, لاية, قالت, مقاطع, وزار, صحة, حوض, مساء, سكان, حالة, امطار]|Disasters|\n",
            "+-----+-------------------------------------------------------------------------------------------------------------------------+---------+\n",
            "\n",
            "   Topic to Category Mapping: {0: 'Politics', 1: 'Unknown', 2: 'Economy', 3: 'Politics', 4: 'Disasters'}\n",
            "5. Categorizing Documents...\n",
            "Pipeline Validation Failed: ArabicNewsTopicModeler.categorize_documents() takes 3 positional arguments but 4 were given\n",
            "\n",
            "Running Full Topic Modeling...\n",
            "+-----+------------------------------------------------------------------------------------------------------------------------------+--------+\n",
            "|topic|top_terms                                                                                                                     |category|\n",
            "+-----+------------------------------------------------------------------------------------------------------------------------------+--------+\n",
            "|0    |[ولد, محمد, رئيس, نواكشوط, اخبار, يوم, غزواني, عبد, حزب, احمد, شيخ, مدير, خلال, امين, عام, مجلس, وطني, اول, سيدي, وزير]       |Politics|\n",
            "|1    |[علي, ان, ما, الي, التي, لا, او, ذلك, الذي, الله, كل, انه, لم, ولا, هو, غير, سياس, دولة, قان, كما]                            |Politics|\n",
            "|2    |[علي, الي, موريتانيا, ان, خلال, بين, التي, دول, قطاع, مجال, مالي, رئيس, بلد, عمل, نواكشوط, زير, موريتاني, اقتصاد, اخبار, حكوم]|Politics|\n",
            "|3    |[ان, علي, الي, اخبار, انه, شركة, قال, التي, ملم, نواكشوط, عمل, يان, تم, اضاف, ادار, بعد, اجراء, انها, مال, سلط]               |Economy |\n",
            "|4    |[نواكشوط, اخبار, علي, الي, يوم, مم, مدين, نواذيبو, عاصم, لاية, ان, جديد, مقاطع, اوق, وزار, زارة, اعلن, وقت, بلاد, عدد]        |Unknown |\n",
            "+-----+------------------------------------------------------------------------------------------------------------------------------+--------+\n",
            "\n",
            "Topic to Category Mapping: {0: 'Politics', 1: 'Politics', 2: 'Politics', 3: 'Economy', 4: 'Unknown'}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ArabicNewsTopicModeler.categorize_documents() takes 3 positional arguments but 4 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-eae2f5456e64>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-eae2f5456e64>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nRunning Full Topic Modeling...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0mmodeler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArabicNewsTopicModeler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m     \u001b[0mmodeler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_topic_modeling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-eae2f5456e64>\u001b[0m in \u001b[0;36mrun_topic_modeling\u001b[0;34m(self, input_path)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;31m# Categorize documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         final_data = self.categorize_documents(\n\u001b[0m\u001b[1;32m    153\u001b[0m             \u001b[0mlda_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorized_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: ArabicNewsTopicModeler.categorize_documents() takes 3 positional arguments but 4 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import concat_ws, col, udf, collect_list, expr\n",
        "from pyspark.sql.types import StringType, ArrayType\n",
        "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
        "from pyspark.ml.clustering import LDA\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ArabicNewsTopicModeling\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "data = spark.read.json(\"alakhbar.json\")\n",
        "print(\"Structure of the loaded dataset:\")\n",
        "data.printSchema()\n",
        "print(\"Preview of the first 5 records:\")\n",
        "data.show(5, truncate=80)\n",
        "\n",
        "\n",
        "data = data.withColumn(\"text\", concat_ws(\" \", col(\"title\"), col(\"description\"), col(\"content\")))\n",
        "\n",
        "def normalize_arabic(text):\n",
        "    \"\"\"Normalize Arabic text by removing diacritics, normalizing forms, and keeping only Arabic characters\"\"\"\n",
        "    if text is None:\n",
        "        return \"\"\n",
        "\n",
        "    text = text.replace(\"أ\", \"ا\").replace(\"إ\", \"ا\").replace(\"آ\", \"ا\")\n",
        "\n",
        "    text = text.replace(\"ى\", \"ي\")\n",
        "\n",
        "    diac_pattern = re.compile(r'[\\u064B-\\u065F\\u0640]')\n",
        "    text = re.sub(diac_pattern, '', text)\n",
        "\n",
        "    arabic_pattern = re.compile(r'[^\\u0600-\\u06FF\\s]')\n",
        "    text = re.sub(arabic_pattern, ' ', text)\n",
        "\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "normalize_udf = udf(normalize_arabic, StringType())\n",
        "data = data.withColumn(\"norm_text\", normalize_udf(col(\"text\")))\n",
        "\n",
        "tokenizer = RegexTokenizer(inputCol=\"norm_text\", outputCol=\"words\", pattern=\"[^\\\\p{L}\\\\p{N}_]+\", gaps=True)\n",
        "tokenized_data = tokenizer.transform(data)\n",
        "\n",
        "\n",
        "arabic_stopwords = [\n",
        "    \"في\", \"من\", \"على\", \"إلى\", \"عن\", \"مع\", \"هذا\", \"هذه\", \"أن\", \"و\", \"ب\", \"ل\", \"الى\",\n",
        "    \"إن\", \"ثم\", \"لكن\", \"أو\", \"كان\", \"كانت\", \"يكون\", \"وكان\", \"وكانت\", \"هو\", \"هي\",\n",
        "    \"كل\", \"بعض\", \"تم\", \"لا\", \"لم\", \"لن\", \"ما\", \"هناك\", \"كما\", \"قال\", \"قالت\", \"يقول\",\n",
        "    \"عندما\", \"عند\", \"حيث\", \"منذ\", \"خلال\", \"بعد\", \"قبل\", \"حتى\", \"اذا\", \"إذا\", \"بين\",\n",
        "    \"منها\", \"منه\", \"له\", \"لها\", \"التي\", \"الذي\", \"فيه\", \"فيها\", \"عنه\", \"عنها\"\n",
        "]\n",
        "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", stopWords=arabic_stopwords)\n",
        "filtered_data = stopwords_remover.transform(tokenized_data)\n",
        "\n",
        "def simple_arabic_stem(word_list):\n",
        "    \"\"\"Simple Arabic stemmer that removes common prefixes and suffixes\"\"\"\n",
        "    if word_list is None:\n",
        "        return []\n",
        "\n",
        "    result = []\n",
        "\n",
        "    for word in word_list:\n",
        "        if len(word) <= 3:\n",
        "            result.append(word)\n",
        "            continue\n",
        "\n",
        "        if word.startswith(\"ال\") and len(word) > 4:\n",
        "            word = word[2:]\n",
        "        elif word.startswith((\"و\", \"ف\", \"ب\", \"ي\", \"ت\")) and len(word) > 3:\n",
        "            word = word[1:]\n",
        "\n",
        "        if len(word) > 4:\n",
        "            if word.endswith((\"ون\", \"ين\", \"ات\", \"ان\")):\n",
        "                word = word[:-2]\n",
        "            elif word.endswith((\"تي\", \"يه\", \"ية\", \"ته\", \"ها\", \"هم\", \"هن\", \"نا\")):\n",
        "                word = word[:-2]\n",
        "            elif word.endswith((\"ه\", \"ة\", \"ت\")):\n",
        "                word = word[:-1]\n",
        "\n",
        "        result.append(word)\n",
        "\n",
        "    return result\n",
        "\n",
        "stem_udf = udf(simple_arabic_stem, ArrayType(StringType()))\n",
        "stemmed_data = filtered_data.withColumn(\"stemmed\", stem_udf(col(\"filtered\")))\n",
        "\n",
        "\n",
        "cv = CountVectorizer(inputCol=\"stemmed\", outputCol=\"features\", vocabSize=10000, minDF=5, minTF=2)\n",
        "cv_model = cv.fit(stemmed_data)\n",
        "vectorized_data = cv_model.transform(stemmed_data)\n",
        "\n",
        "\n",
        "lda = LDA(k=5, maxIter=50, featuresCol=\"features\", seed=42, optimizer='em', learningDecay=0.5)\n",
        "lda_model = lda.fit(vectorized_data)\n",
        "\n",
        "\n",
        "vocab = cv_model.vocabulary\n",
        "\n",
        "print(\"Most representative keywords for each identified topic:\")\n",
        "topics = lda_model.describeTopics(maxTermsPerTopic=20)\n",
        "topics_with_terms = topics.rdd.map(\n",
        "    lambda topic: (\n",
        "        topic.topic,\n",
        "        [vocab[idx] for idx in topic.termIndices]\n",
        "    )\n",
        ").toDF([\"topic\", \"top_terms\"])\n",
        "topics_with_terms.show(truncate=False)\n",
        "\n",
        "category_keywords = {\n",
        "    \"Politics\": [\n",
        "        \"رئيس\", \"وزير\", \"حكومة\", \"انتخابات\", \"سياسة\", \"برلمان\", \"دولة\", \"قرار\", \"سفير\", \"خارجية\",\n",
        "        \"رئاسة\", \"مجلس\", \"سلطة\", \"دبلوماسية\", \"سياسي\", \"زيارة\", \"قمة\", \"اجتماع\", \"وفد\", \"سفارة\",\n",
        "        \"دستور\", \"تصريح\", \"عهد\", \"امير\", \"ملك\", \"سلطان\", \"امن\", \"رئاسي\", \"وزارة\", \"حزب\", \"نائب\"\n",
        "    ],\n",
        "    \"Economy\": [\n",
        "        \"اقتصاد\", \"سوق\", \"مال\", \"شركة\", \"بنك\", \"استثمار\", \"مليون\", \"دولار\", \"تجارة\", \"ميزانية\",\n",
        "        \"تمويل\", \"اسهم\", \"مصرف\", \"انتاج\", \"سعر\", \"اسعار\", \"صناعة\", \"نفط\", \"ثروة\", \"معدن\",\n",
        "        \"ذهب\", \"منجم\", \"مشروع\", \"تنمية\", \"سندات\", \"صندوق\", \"راس مال\", \"تضخم\", \"عملة\", \"بورصة\"\n",
        "    ],\n",
        "    \"Sports\": [\n",
        "        \"مباراة\", \"لاعب\", \"فريق\", \"كرة\", \"منتخب\", \"بطولة\", \"دوري\", \"هدف\", \"مدرب\", \"فوز\",\n",
        "        \"قدم\", \"سلة\", \"يد\", \"طائرة\", \"سباق\", \"ملعب\", \"استاد\", \"تدريب\", \"مونديال\", \"اولمبياد\",\n",
        "        \"كاس\", \"جولة\", \"رياضي\", \"نادي\", \"مهاجم\", \"مدافع\", \"حارس\", \"مرمى\", \"تسجيل\", \"احراز\"\n",
        "    ],\n",
        "    \"Disasters\": [\n",
        "        \"امطار\", \"سيول\", \"فيضان\", \"عاصفة\", \"اعصار\", \"جفاف\", \"حريق\", \"كارثة\", \"ضحايا\", \"سد\",\n",
        "        \"انهيار\", \"غرق\", \"طوفان\", \"تضرر\", \"خسائر\", \"منكوبة\", \"اضرار\", \"انقاذ\", \"اغاثة\", \"طوارئ\",\n",
        "        \"مياه\", \"محاصر\", \"حادث\", \"وفاة\", \"مصاب\", \"اصابة\", \"كسر\", \"تحطم\", \"مطر\", \"رياح\"\n",
        "    ],\n",
        "    \"Social\": [\n",
        "        \"مجتمع\", \"صحة\", \"تعليم\", \"مدرسة\", \"طلاب\", \"مستشفى\", \"مرض\", \"دواء\", \"جامعة\", \"عائلة\",\n",
        "        \"طفل\", \"اطفال\", \"نساء\", \"رجال\", \"شباب\", \"كبار\", \"خدمات\", \"تطوع\", \"حملة\", \"توعية\",\n",
        "        \"فعالية\", \"مبادرة\", \"مهرجان\", \"احتفال\", \"منظمة\", \"جمعية\", \"هيئة\", \"مؤسسة\", \"عامة\", \"ثقافة\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "def assign_category(top_terms):\n",
        "    if not top_terms:\n",
        "        return \"Unknown\"\n",
        "\n",
        "    scores = {}\n",
        "    for category, keywords in category_keywords.items():\n",
        "        score = sum(1 for term in top_terms if term in keywords)\n",
        "        scores[category] = score\n",
        "\n",
        "    if max(scores.values()) > 0:\n",
        "        return max(scores.items(), key=lambda x: x[1])[0]\n",
        "    else:\n",
        "        return \"Unknown\"\n",
        "\n",
        "assign_category_udf = udf(assign_category, StringType())\n",
        "\n",
        "labeled_topics = topics_with_terms.withColumn(\"category\", assign_category_udf(col(\"top_terms\")))\n",
        "print(\"Semantic categories assigned to each discovered topic:\")\n",
        "labeled_topics.show(truncate=False)\n",
        "\n",
        "topic_to_category_df = labeled_topics.select(\"topic\", \"category\")\n",
        "topic_to_category = {row[\"topic\"]: row[\"category\"] for row in topic_to_category_df.collect()}\n",
        "print(\"Dictionary mapping topic numbers to their content categories:\", topic_to_category)\n",
        "\n",
        "topics_data = lda_model.transform(vectorized_data)\n",
        "\n",
        "def get_dominant_topic(topic_distribution):\n",
        "    \"\"\"Get the index of the topic with highest probability\"\"\"\n",
        "    return int(np.argmax(topic_distribution))\n",
        "\n",
        "get_dominant_topic_udf = udf(get_dominant_topic, StringType())\n",
        "\n",
        "categorized_data = topics_data.withColumn(\"dominant_topic\", get_dominant_topic_udf(col(\"topicDistribution\")))\n",
        "\n",
        "\n",
        "def map_topic_to_category(topic_id):\n",
        "    \"\"\"Map topic ID to the assigned category\"\"\"\n",
        "    return topic_to_category.get(int(topic_id), \"Unknown\")\n",
        "\n",
        "\n",
        "map_topic_udf = udf(map_topic_to_category, StringType())\n",
        "\n",
        "\n",
        "final_data = categorized_data.withColumn(\"category\", map_topic_udf(col(\"dominant_topic\")))\n",
        "\n",
        "\n",
        "print(\"Example articles with their automatically detected categories:\")\n",
        "final_data.select(\"id\", \"title\", \"dominant_topic\", \"category\").show(10, truncate=80)\n",
        "\n",
        "\n",
        "category_distribution = final_data.groupBy(\"category\").count().orderBy(\"count\", ascending=False)\n",
        "print(\"Summary of article count in each thematic category:\")\n",
        "category_distribution.show()\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import to_timestamp\n",
        "data_with_timestamp = data.withColumn(\"timestamp\", to_timestamp(col(\"pub_date\")))\n",
        "recent_articles = data_with_timestamp.orderBy(col(\"timestamp\").desc()).limit(2)\n",
        "\n",
        "recent_tokenized = tokenizer.transform(recent_articles)\n",
        "recent_filtered = stopwords_remover.transform(recent_tokenized)\n",
        "recent_stemmed = recent_filtered.withColumn(\"stemmed\", stem_udf(col(\"filtered\")))\n",
        "recent_vectorized = cv_model.transform(recent_stemmed)\n",
        "recent_topics = lda_model.transform(recent_vectorized)\n",
        "\n",
        "recent_categorized = recent_topics \\\n",
        "    .withColumn(\"dominant_topic\", get_dominant_topic_udf(col(\"topicDistribution\"))) \\\n",
        "    .withColumn(\"category\", map_topic_udf(col(\"dominant_topic\")))\n",
        "\n",
        "print(\"Topic classification for most recently published articles:\")\n",
        "recent_categorized.select(\"id\", \"title\", \"dominant_topic\", \"category\", \"pub_date\").show(truncate=80)\n",
        "\n",
        "lda_model.save(\"arabic_news_lda_model\")\n",
        "cv_model.save(\"arabic_news_cv_model\")\n",
        "print(\"LDA model and CountVectorizer successfully persisted to disk.\")\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXFzRwicn68J",
        "outputId": "38e988a0-718a-41b5-a686-8c246028df9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Structure of the loaded dataset:\n",
            "root\n",
            " |-- content: string (nullable = true)\n",
            " |-- description: string (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- link: string (nullable = true)\n",
            " |-- pub_date: string (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            "\n",
            "Preview of the first 5 records:\n",
            "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+-----+-----------------------------------+-------------------------+------------------------------------------------------------------------+\n",
            "|                                                                         content|                                                                     description|   id|                               link|                 pub_date|                                                                   title|\n",
            "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+-----+-----------------------------------+-------------------------+------------------------------------------------------------------------+\n",
            "|لأخبار (نواكشوط) استأنف  قطار الشركة الوطنية للصناعة والمناجم اسنيم قبل أقل م...|لأخبار (نواكشوط) استأنف قطار الشركة الوطنية للصناعة والمناجم اسنيم قبل أقل من...|41129|https://alakhbar.info/?q=node/41129|2022-06-21T00:09:59+00:00|                              بعد إصلاح عطب بالسكك قطار اسنيم يعود للعمل|\n",
            "|الأخبار (نواكشوط) ـ أعلنت اللجنة الفنية المكلفة بمتابعة أعمال اللجنة الوزارية...|الأخبار (نواكشوط) ـ أعلنت اللجنة الفنية المكلفة بمتابعة أعمال اللجنة الوزارية...|41131|https://alakhbar.info/?q=node/41131|2022-06-21T09:13:54+00:00|                 أضرار في الطرق والمساكن بخمس ولايات إثر الأمطار والسيول|\n",
            "|الأخبار (وادان) – عزلت السيول التي تدفقت صباح اليوم من المرتفعات الجبلية مدين...|الأخبار (وادان) – عزلت السيول التي تدفقت صباح اليوم من المرتفعات الجبلية مدين...|41132|https://alakhbar.info/?q=node/41132|2022-06-21T09:51:03+00:00|                  السيول تعزل مدينة ودان وتوقف تنقل السكان منها أو إليها|\n",
            "|الأخبار (وادان) – وصف رئيس حي \"كَافي\" بضاحية مدينة ودان التاريخية اسلاكه ولد ...|الأخبار (وادان) – وصف رئيس حي \"كَافي\" بضاحية مدينة ودان التاريخية اسلاكه ولد ...|41133|https://alakhbar.info/?q=node/41133|2022-06-21T09:54:31+00:00|رئيس حي \"كَافي\" بوادان: نحن محاصرون وهناك أسر منكوبة بسبب السيول (فيديو)|\n",
            "|الأخبار (وادان) – عزلت السيول التي تدفقت صباح اليوم من المرتفعات الجبلية مدين...|الأخبار (وادان) – عزلت السيول التي تدفقت صباح اليوم من المرتفعات الجبلية مدين...|41134|https://alakhbar.info/?q=node/41134|2022-06-21T10:57:00+00:00|          السيول تعزل مدينة ودان وتوقف تنقل السكان منها أو إليها (فيديو)|\n",
            "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+-----+-----------------------------------+-------------------------+------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Most representative keywords for each identified topic:\n",
            "+-----+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "|topic|top_terms                                                                                                                          |\n",
            "+-----+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "|0    |[علي, الي, ان, موريتانيا, رئيس, دول, اخبار, مالي, مجال, اقتصاد, زير, مجموع, بلد, خارج, شركة, نواكشوط, حكوم, افريقيا, قطاع, موريتان]|\n",
            "|1    |[ولد, محمد, رئيس, علي, ان, عبد, احمد, سابق, مدير, شيخ, عام, قان, ادار, مجلس, عامة, عزيز, انه, محكم, الي, سيدي]                     |\n",
            "|2    |[نواكشوط, اخبار, يوم, الي, مم, مدين, علي, نواذيبو, عاصم, لاية, اعلن, مقاطع, زارة, وزار, اوق, ان, وقت, جديد, مساء, يما]             |\n",
            "|3    |[ان, علي, الي, او, ذلك, الله, انه, غير, ولا, ملم, دولة, اي, عليه, وهو, جميع, حتي, دون, الا, وفي, وقد]                              |\n",
            "|4    |[نواكشوط, اخبار, علي, ولد, ان, حزب, غزواني, وطني, رئيس, انتخاب, الي, وطن, عمل, يوم, اجتماع, لجنة, اضاف, تعليم, موريتاني, حكوم]     |\n",
            "+-----+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "Semantic categories assigned to each discovered topic:\n",
            "+-----+-----------------------------------------------------------------------------------------------------------------------------------+--------+\n",
            "|topic|top_terms                                                                                                                          |category|\n",
            "+-----+-----------------------------------------------------------------------------------------------------------------------------------+--------+\n",
            "|0    |[علي, الي, ان, موريتانيا, رئيس, دول, اخبار, مالي, مجال, اقتصاد, زير, مجموع, بلد, خارج, شركة, نواكشوط, حكوم, افريقيا, قطاع, موريتان]|Economy |\n",
            "|1    |[ولد, محمد, رئيس, علي, ان, عبد, احمد, سابق, مدير, شيخ, عام, قان, ادار, مجلس, عامة, عزيز, انه, محكم, الي, سيدي]                     |Politics|\n",
            "|2    |[نواكشوط, اخبار, يوم, الي, مم, مدين, علي, نواذيبو, عاصم, لاية, اعلن, مقاطع, زارة, وزار, اوق, ان, وقت, جديد, مساء, يما]             |Unknown |\n",
            "|3    |[ان, علي, الي, او, ذلك, الله, انه, غير, ولا, ملم, دولة, اي, عليه, وهو, جميع, حتي, دون, الا, وفي, وقد]                              |Politics|\n",
            "|4    |[نواكشوط, اخبار, علي, ولد, ان, حزب, غزواني, وطني, رئيس, انتخاب, الي, وطن, عمل, يوم, اجتماع, لجنة, اضاف, تعليم, موريتاني, حكوم]     |Politics|\n",
            "+-----+-----------------------------------------------------------------------------------------------------------------------------------+--------+\n",
            "\n",
            "Dictionary mapping topic numbers to their content categories: {0: 'Economy', 1: 'Politics', 2: 'Unknown', 3: 'Politics', 4: 'Politics'}\n",
            "Example articles with their automatically detected categories:\n",
            "+-----+------------------------------------------------------------------------+--------------+--------+\n",
            "|   id|                                                                   title|dominant_topic|category|\n",
            "+-----+------------------------------------------------------------------------+--------------+--------+\n",
            "|41129|                              بعد إصلاح عطب بالسكك قطار اسنيم يعود للعمل|             2| Unknown|\n",
            "|41131|                 أضرار في الطرق والمساكن بخمس ولايات إثر الأمطار والسيول|             2| Unknown|\n",
            "|41132|                  السيول تعزل مدينة ودان وتوقف تنقل السكان منها أو إليها|             2| Unknown|\n",
            "|41133|رئيس حي \"كَافي\" بوادان: نحن محاصرون وهناك أسر منكوبة بسبب السيول (فيديو)|             2| Unknown|\n",
            "|41134|          السيول تعزل مدينة ودان وتوقف تنقل السكان منها أو إليها (فيديو)|             2| Unknown|\n",
            "|41135|                  منتخبو وادان: السلطات تسرعت في تقييمها لحجم الأضرار   |             2| Unknown|\n",
            "|41136|           نقابيون وبحارة يحتجون على إعفاء مدير الدائرة البحرية بنواذيبو|             1|Politics|\n",
            "|41137|          مالي تبلغ مواطنيها رفع اشتراط رخصة الدخول للعبور إلى موريتانيا|             0| Economy|\n",
            "|41138|                 غزواني: نتمتع بموقع استراتيجي ممتاز ومنفتحون على أوروبا|             0| Economy|\n",
            "|41139|                     الهابا تطلق دورة تكوينية حول الصحافة والتحدي الرقمي|             4|Politics|\n",
            "+-----+------------------------------------------------------------------------+--------------+--------+\n",
            "only showing top 10 rows\n",
            "\n",
            "Summary of article count in each thematic category:\n",
            "+--------+-----+\n",
            "|category|count|\n",
            "+--------+-----+\n",
            "|Politics|15730|\n",
            "| Economy| 8343|\n",
            "| Unknown| 5579|\n",
            "+--------+-----+\n",
            "\n",
            "Topic classification for most recently published articles:\n",
            "+-----+-----------------------------------------------------------+--------------+--------+-------------------------+\n",
            "|   id|                                                      title|dominant_topic|category|                 pub_date|\n",
            "+-----+-----------------------------------------------------------+--------------+--------+-------------------------+\n",
            "|58999|وزير الدفاع يغادر إلى التشيك ضمن جولة خارجية تستمر 8 أيام  |             0| Economy|2025-02-11T14:18:53+00:00|\n",
            "|58998| موريتانيا والكويت تبحثان التعاون في إدارة المخاطر والكوارث|             0| Economy|2025-02-11T12:06:13+00:00|\n",
            "+-----+-----------------------------------------------------------+--------------+--------+-------------------------+\n",
            "\n",
            "LDA model and CountVectorizer successfully persisted to disk.\n"
          ]
        }
      ]
    }
  ]
}